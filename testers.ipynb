{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Ignore this code cell, this is an alternate variation I was messing around with at first, start below\"\"\"\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "# Replace the values below with your own API key and API secret key\n",
    "api_key = \"\"\n",
    "api_secret_key = \"\"\n",
    "\n",
    "# Encode the API key and API secret key in base64\n",
    "encoded_api_key_secret = base64.b64encode(f\"{api_key}:{api_secret_key}\".encode()).decode()\n",
    "\n",
    "# Obtain a Bearer token\n",
    "headers = {\n",
    "    \"Authorization\": f\"Basic {encoded_api_key_secret}\",\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded;charset=UTF-8\"\n",
    "}\n",
    "data = {\n",
    "    \"grant_type\": \"client_credentials\"\n",
    "}\n",
    "response = requests.post(\"https://api.twitter.com/oauth2/token\", headers=headers, data=data)\n",
    "\n",
    "# Extract the Bearer token from the response\n",
    "bearer_token = response.json()[\"access_token\"]\n",
    "\n",
    "# Use the Bearer token to authenticate requests to the Twitter API\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {bearer_token}\"\n",
    "}\n",
    "\n",
    "# Retrieve the most recent 100 tweets containing the word \"NFT\"\n",
    "response = requests.get(\"https://api.twitter.com/2/tweets/search/recent\", headers=headers, params={\n",
    "    \"query\": \"NFT\",\n",
    "    \"max_results\": 100\n",
    "})\n",
    "\n",
    "# Extract the tweets from the response\n",
    "tweets = response.json()[\"data\"]\n",
    "\n",
    "# Print the text of each tweet\n",
    "#for tweet in tweets:\n",
    " #   print(tweet[\"text\"])\n",
    "\n",
    "df = pd.DataFrame(tweets)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "# To set your enviornment variables in your terminal run the following line:\n",
    "# export 'BEARER_TOKEN'='<your_bearer_token>'\n",
    "bearer_token = \"\"\n",
    "\n",
    "\n",
    "def create_url():\n",
    "    # User fields are adjustable, options include:\n",
    "    # created_at, description, entities, id, location, name,\n",
    "    # pinned_tweet_id, profile_image_url, protected,\n",
    "    # public_metrics, url, username, verified, and withheld\n",
    "    user_fields = \"user.fields=created_at,description\"\n",
    "    # You can replace the ID given with the Tweet ID you wish to like.\n",
    "    # You can find an ID by using the Tweet lookup endpoint\n",
    "    id = \"1602742534366601218\"\n",
    "    # You can adjust ids to include a single Tweets.\n",
    "    # Or you can add to up to 100 comma-separated IDs\n",
    "    url = \"https://api.twitter.com/2/tweets/{}/liking_users\".format(id)\n",
    "    return url, user_fields\n",
    "\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2LikingUsersPython\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, user_fields):\n",
    "    response = requests.request(\"GET\", url, auth=bearer_oauth, params=user_fields)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Request returned an error: {} {}\".format(\n",
    "                response.status_code, response.text\n",
    "            )\n",
    "        )\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def main():\n",
    "    url, tweet_fields = create_url()\n",
    "    json_response = connect_to_endpoint(url, tweet_fields)\n",
    "    print(json.dumps(json_response, indent=4, sort_keys=True))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs for tweets\n",
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "keyword = \"xbox lang:en\"\n",
    "start_list = [\"2023-01-15T17:00:00Z\", \"2023-01-16T17:00:00Z\", \"2023-01-17T17:00:00Z\"]\n",
    "end_list = [\"2023-01-15T18:00:00Z\", \"2023-01-16T18:00:00Z\", \"2023-01-17T18:00:00Z\"]\n",
    "max_results = 500\n",
    "\n",
    "#Total number of tweets we collected from the loop\n",
    "total_tweets = 0\n",
    "\n",
    "# Create file\n",
    "csvFile = open(\"data.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "#Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n",
    "csvWriter.writerow(['author id', 'created_at', 'geo', 'id','lang', 'like_count', 'quote_count', 'reply_count','retweet_count','source','tweet'])\n",
    "csvFile.close()\n",
    "\n",
    "for i in range(0,len(start_list)):\n",
    "\n",
    "    # Inputs\n",
    "    count = 0 # Counting tweets per time period\n",
    "    max_count = 100 # Max tweets per time period\n",
    "    flag = True\n",
    "    next_token = None\n",
    "    \n",
    "    # Check if flag is true\n",
    "    while flag:\n",
    "        # Check if max_count reached\n",
    "        if count >= max_count:\n",
    "            break\n",
    "        print(\"-------------------\")\n",
    "        print(\"Token: \", next_token)\n",
    "        url = create_url(keyword, start_list[i],end_list[i], max_results)\n",
    "        json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "        result_count = json_response['meta']['result_count']\n",
    "\n",
    "        if 'next_token' in json_response['meta']:\n",
    "            # Save the token to use for next call\n",
    "            next_token = json_response['meta']['next_token']\n",
    "            print(\"Next Token: \", next_token)\n",
    "            if result_count is not None and result_count > 0 and next_token is not None:\n",
    "                print(\"Start Date: \", start_list[i])\n",
    "                write_to_csv(json_response, \"data.csv\")\n",
    "                count += result_count\n",
    "                total_tweets += result_count\n",
    "                print(\"Total # of Tweets added: \", total_tweets)\n",
    "                print(\"-------------------\")\n",
    "                time.sleep(5)                \n",
    "        # If no next token exists\n",
    "        else:\n",
    "            if result_count is not None and result_count > 0:\n",
    "                print(\"-------------------\")\n",
    "                print(\"Start Date: \", start_list[i])\n",
    "                write_to_csv(json_response, \"data.csv\")\n",
    "                count += result_count\n",
    "                total_tweets += result_count\n",
    "                print(\"Total # of Tweets added: \", total_tweets)\n",
    "                print(\"-------------------\")\n",
    "                time.sleep(5)\n",
    "            \n",
    "            #Since this is the final request, turn flag to false to move to the next time period.\n",
    "            flag = False\n",
    "            next_token = None\n",
    "        time.sleep(5)\n",
    "print(\"Total number of results: \", total_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "\n",
    "\n",
    "def tweets_per_hour_range(keyword: str):\n",
    "    \"\"\"This function returns 10,000 tweets (containing keyword) per 1 hour range from 100 random hour ranges in the past year.\"\"\"\n",
    "    bearer_token = auth()\n",
    "    headers = create_headers(bearer_token)\n",
    "    # Creates 100 random one hour ranges in the past year\n",
    "    \n",
    "    start_list = list()\n",
    "    end_list = list()\n",
    "    for _ in range(0, 100):\n",
    "        start_time, end_time = random_date()\n",
    "        while start_time in start_list:\n",
    "            start_time, end_time = random_date()\n",
    "        start_list.append(start_time)\n",
    "        end_list.append(end_time)\n",
    "    \n",
    "    # Counts total tweets, not tweets per time period\n",
    "    total_tweets: int = 0\n",
    "    # Max results per enpoint call, system limit is 100\n",
    "    max_results: int = 100 \n",
    "\n",
    "    # Create file data.csv\n",
    "    csvFile = open(\"data.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n",
    "    csvWriter.writerow(['author id', 'created_at', 'id', 'like_count', 'reply_count','retweet_count', 'source', 'tweet'])\n",
    "    csvFile.close()\n",
    "\n",
    "    for i in range(0, len(start_list)):\n",
    "\n",
    "        # Inputs\n",
    "        count: int = 0 # Counts tweets per time period\n",
    "        max_count: int = 1 # Input for max tweets per time period\n",
    "        # Total tweets will be max_count times the amount of time ranges\n",
    "        flag: bool = True\n",
    "        next_token: str = None\n",
    "    \n",
    "        # Check if flag is true\n",
    "        while flag:\n",
    "            # Check if max_count reached\n",
    "            if count >= max_count:\n",
    "                break\n",
    "            print(\"-------------------\")\n",
    "            print(\"Token: \", next_token)\n",
    "            url = create_url(keyword, start_list[i],end_list[i], max_results)\n",
    "            json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "            result_count = json_response['meta']['result_count']\n",
    "\n",
    "            if 'next_token' in json_response['meta']:\n",
    "                # Save the token to use for next call\n",
    "                next_token = json_response['meta']['next_token']\n",
    "                print(\"Next Token: \", next_token)\n",
    "                if result_count is not None and result_count > 0 and next_token is not None:\n",
    "                    #write_to_csv('data.json', 'data.csv')\n",
    "                    write_to_csv(json_response, 'data.csv')\n",
    "                    count += result_count\n",
    "                    total_tweets += result_count\n",
    "                    print(\"Total # of Tweets added: \", total_tweets)\n",
    "                    print(\"-------------------\")\n",
    "                    time.sleep(5)                \n",
    "            # If no next token exists\n",
    "            else:\n",
    "                if result_count is not None and result_count > 0:\n",
    "                    print(\"-------------------\")\n",
    "                    print(\"Start Date: \", start_list[i])\n",
    "                    #write_to_csv('data.json', 'data.csv')\n",
    "                    write_to_csv(json_response, 'data.csv')\n",
    "                    count += result_count\n",
    "                    total_tweets += result_count\n",
    "                    print(\"Total # of Tweets added: \", total_tweets)\n",
    "                    print(\"-------------------\")\n",
    "                    time.sleep(5)\n",
    "            \n",
    "                #Since this is the final request, turn flag to false to move to the next time period.\n",
    "                flag = False\n",
    "                next_token = None\n",
    "            time.sleep(5)\n",
    "    print(\"Total number of results: \", total_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil\n",
    "'''\n",
    "def append_to_csv(json_response, fileName):\n",
    "\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "        \n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # So we will account for that\n",
    "\n",
    "        # 1. Author ID\n",
    "        author_id = tweet['author_id']\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # 3. Geolocation\n",
    "        if ('geo' in tweet):   \n",
    "            geo = tweet['geo']['place_id']\n",
    "        else:\n",
    "            geo = \" \"\n",
    "\n",
    "        # 4. Tweet ID\n",
    "        tweet_id = tweet['id']\n",
    "\n",
    "        # 5. Language\n",
    "        lang = tweet['lang']\n",
    "\n",
    "        # 6. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "        # 7. source\n",
    "        source = tweet['source']\n",
    "\n",
    "        # 8. Tweet text\n",
    "        text = tweet['text']\n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [author_id, created_at, geo, tweet_id, lang, like_count, quote_count, reply_count, retweet_count, source, text]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"WORKING EXAMPLE OF SO FAR, Above is for more function use etc.\"\"\"\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "bearer_token = auth()\n",
    "\"\"\"input the necessary inputs below.\"\"\"\n",
    "headers = create_headers(bearer_token)\n",
    "keyword = \"Smartchain OR Airdrop OR Crypto\"\n",
    "# Use the commented out code below when using full access api\n",
    "#start_time, end_time = random_date()\n",
    "start_time = \"2023-01-02T17:00:00Z\"\n",
    "end_time = \"2023-01-04T20:00:00Z\"\n",
    "\n",
    "max_results = 100\n",
    "url = create_url(keyword, start_time, end_time, max_results)\n",
    "json_response = connect_to_endpoint(url[0], headers, url[1])\n",
    "result_dict = json_response[\"data\"]\n",
    "#while \"next_token\" in json_response[\"meta\"] and len(result_dict) < 9900:\n",
    "    #print(json.dumps(json_response, indent=4, sort_keys=True))\n",
    "    #json_response = connect_to_endpoint(url[0], headers, url[1], json_response[\"meta\"][\"next_token\"])\n",
    "    #result_dict.extend(json_response[\"data\"])\n",
    "#print(json.dumps(json_response, indent=4, sort_keys=True))\n",
    "print(json.dumps(result_dict, indent=4, sort_keys=True))\n",
    "\n",
    "#def append_to_csv(json_response, fileName):\n",
    "with open('data.json', 'w') as f:\n",
    "    json.dump(json_response, f)\n",
    "#df = pd.read_json('data.json')\n",
    "write_to_csv()\n",
    "#url = create_url(keyword, start_time, end_time, max_results)\n",
    "#json_response = connect_to_endpoint(url[0], headers, url[1])\n",
    "#print(json.dumps(json_response, indent=4, sort_keys=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (v3.11.0:deaf509e8f, Oct 24 2022, 14:43:23) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
