{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First protocol\n",
    "_Code written and runs in python 3.11.0. Modify environment variables and queries as needed._  \n",
    "_Please use venv_\n",
    "\n",
    "## Protocol\n",
    "· Start with keywords:\n",
    "\n",
    "- Smartchain\n",
    "\n",
    "- Nft\n",
    "\n",
    "- Airdrop\n",
    "\n",
    "- Crypto\n",
    "\n",
    "- …etc.\n",
    "\n",
    "1. Sample up to 10k tweets containing at least one term from 100 random hours from the past year (so 1M tweets)\n",
    "\n",
    "2. Determine the most engaged (top) with users from this combined sample (100 or 1000)\n",
    "\n",
    "3. Pull up to 1000 comments for each top user\n",
    "\n",
    "4. Determine top users whose comments mention at least three users other than the top user\n",
    "\n",
    "5. Expand top user sample if we don’t have at least 100 airdrop seeders\n",
    "\n",
    "6. Time series chart plots:\n",
    "\n",
    "7. Top user activity\n",
    "\n",
    "8. Airdrop seeder activity\n",
    "\n",
    "9. Negative reaction activity? (based on sentiment analysis of replies to airdrop messages)\n",
    "\n",
    "10. External crypto value signals (from where?)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "Run the following commands in the terminal to install the required packages\n",
    "\n",
    "$pip install requests  \n",
    "  \n",
    "$pip install pandas  \n",
    "  \n",
    "$pip install datetime  \n",
    "  \n",
    "$pip install python-dateutil\n",
    "  \n",
    "\n",
    "--------------------  \n",
    "\n",
    "Crypto packages/api's:\n",
    "https://medium.com/codex/10-best-resources-to-fetch-cryptocurrency-data-in-python-8400cf0d0136\n",
    "\n",
    "https://www.alphavantage.co/documentation/\n",
    "\n",
    "\n",
    "Financial tools/packages:\n",
    "https://twitter.com/pyquantnews/status/1568029967052640256?t=EthvrNWmYhAFDVOhRoDxrQ&s=03\n",
    "\n",
    "https://pmorissette.github.io/ffn/quick.html#data-retrieval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Authentication step\n",
    "In the code cell below replace bearer_token with your bearer token. Run the cell, then delete your bearer token.\n",
    "This creates the token as an environment variable to be used under the name TOKEN. The token can then be removed so that others do not have access to your token when code is shared via GitHub. I will change this to dotenv and a .gitignore file later I just havent done that yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKEN'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "from typing import Optional\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import requests\n",
    "from statistics import mean\n",
    "\n",
    "\"\"\"This code creates functions to be used for authentication as well as creating endpoints.\"\"\"\n",
    "def auth():\n",
    "    \"\"\"Retrieves your bearer token.\"\"\"\n",
    "    return os.getenv('TOKEN')\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    \"\"\"Creates headers for proper authentication from an API request.\"\"\"\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def create_full_search_url(keyword: str, start_date: str, end_date: str, max_results: int = 100):\n",
    "    \"\"\"Creates queries and params for a full archive search url.\"\"\"\n",
    "    search_url: str = \"https://api.twitter.com/2/tweets/search/all\" \n",
    "\n",
    "    # change params to desired params\n",
    "    query_params: dict = {'query': keyword,\n",
    "                    'start_time': start_date,\n",
    "                    'end_time': end_date,\n",
    "                    'max_results': max_results,\n",
    "                    'expansions': 'referenced_tweets.id.author_id',\n",
    "                    'tweet.fields': 'id,author_id,conversation_id,created_at,in_reply_to_user_id,lang,public_metrics,referenced_tweets,source,text',\n",
    "                    #'user.fields': 'id,name,public_metrics,username,verified',\n",
    "                    #'place.fields': 'country',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)\n",
    "\n",
    "def get_author_id_from_tweet_id(tweet_id: str):\n",
    "    \"\"\"Creates queries to find the author id of a tweet's author.\"\"\"\n",
    "    search_url: str = f\"https://api.twitter.com/2/tweets/{tweet_id}\" \n",
    "\n",
    "    # change params to desired params\n",
    "    query_params: dict = {'tweet.fields': 'author_id'}\n",
    "    b_tok = auth()\n",
    "    headers = create_headers(b_tok)\n",
    "    response = requests.request(\"GET\", search_url, headers = headers, params = query_params)\n",
    "    print(response.status_code)\n",
    "    json_response = response.json()\n",
    "    if 'data' in json_response:\n",
    "        return(response.json()['data']['author_id'])\n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    \"\"\"This takes a url from a url creation function and the params from the same function as well as an optional next token and returns a json object response from the endpoint.\"\"\"\n",
    "    params['next_token'] = next_token   # if a next token is found this will assign it to params 'next_token' key\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"\\nEndpoint Response Code: \" + str(response.status_code)) # prints the enpoint response code for debugging help\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "\"\"\"This code cell contains two functions (is_leap_year and random_date) which help generate a random one hour date range when random_date() is called\"\"\"\n",
    "def is_leap_year(year: int):\n",
    "    \"\"\"Returns True if the given year in typical four digit year format (i.e. 2023) is a leap year, False otherwise.\"\"\"\n",
    "    if year % 4 == 0:\n",
    "        if year % 100 == 0:\n",
    "            if year % 400 == 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def sort_timestamps(timestamps: list):\n",
    "    \"\"\"This function takes the list of start and end times in rfc 3339 format, returned from the return_n_random_hour_ranges_sorted function, and sorts the lists in chronological order.\"\"\"\n",
    "    # Convert timestamps to datetime objects\n",
    "    datetimes = [datetime.datetime.fromisoformat(ts) for ts in timestamps]\n",
    "    # Sort datetime objects\n",
    "    datetimes.sort()\n",
    "    # Convert sorted datetime objects back to timestamps\n",
    "    sorted_timestamps = [dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\") for dt in datetimes]\n",
    "    return sorted_timestamps\n",
    "\n",
    "def random_date():\n",
    "    \"\"\"Generate a random one hour date range within the last year in RFC 3339 format to be used with twitter API.\"\"\"\n",
    "    month = random.randint(1, 12)\n",
    "    year = random.randint(datetime.datetime.now().year - 1, datetime.datetime.now().year)\n",
    "    if month <= datetime.datetime.now().month:\n",
    "        year = datetime.datetime.now().year\n",
    "    else:\n",
    "        year = datetime.datetime.now().year - 1\n",
    "    if month == datetime.datetime.now().month:\n",
    "        if datetime.datetime.now().day <= 2:\n",
    "            day = 1\n",
    "        else:\n",
    "            day = random.randint(1, datetime.datetime.now().day - 1)\n",
    "    elif month == 2:\n",
    "        if is_leap_year(year):\n",
    "            day = random.randint(1, 29)\n",
    "        else:\n",
    "            day = random.randint(1, 28)\n",
    "    elif month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "        day = random.randint(1, 31)\n",
    "    else:\n",
    "        day = random.randint(1, 30)\n",
    "    hour = random.randint(0, 23)\n",
    "    start_time = datetime.datetime(year, month, day, hour)\n",
    "    end_time = start_time + datetime.timedelta(hours=1)\n",
    "    start_timestamp = start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    end_timestamp = end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    return start_timestamp, end_timestamp\n",
    "\n",
    "def list_of_one_day_ranges_for_rfc_3339_past_year():\n",
    "    \"\"\"Generates 365 one day ranges in rfc3339 format starting at the specified date.\"\"\"\n",
    "    start_time_list: list = list()\n",
    "    end_time_list: list = list()\n",
    "    time1 = datetime.datetime.now()\n",
    "    time1 = time1.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    time1 = time1 - datetime.timedelta(days=365)\n",
    "    #time1 = datetime.datetime(2022, 3, 1, 0)\n",
    "    time1 = time1.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    for i in range(365):\n",
    "        start_time_list.append(time1)\n",
    "        time2 = add_day_to_timestamp(time1)\n",
    "        end_time_list.append(time2)\n",
    "        time1 = add_day_to_timestamp(time1)\n",
    "        \n",
    "    return start_time_list, end_time_list\n",
    "        \n",
    "def list_of_one_day_ranges_for_rfc_3339_since_specific_date():\n",
    "    \"\"\"Generates 365 one day ranges in rfc3339 format starting at the specified date.\"\"\"\n",
    "    start_time_list: list = list()\n",
    "    end_time_list: list = list()\n",
    "    time1 = datetime.datetime(2023, 2, 1)\n",
    "    time1 = time1.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    time1 = time1 - datetime.timedelta(days=365)\n",
    "    #time1 = datetime.datetime(2022, 3, 1, 0)\n",
    "    time1 = time1.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    for i in range(365):\n",
    "        start_time_list.append(time1)\n",
    "        time2 = add_day_to_timestamp(time1)\n",
    "        end_time_list.append(time2)\n",
    "        time1 = add_day_to_timestamp(time1)\n",
    "\n",
    "    return start_time_list, end_time_list\n",
    "\n",
    "def return_n_random_hour_ranges_sorted(n: int) -> list:\n",
    "    \"\"\"This returns two SORTED lists of start times and end times (where each index of the end time is one hour after the same index in start time).\"\"\"\n",
    "    \"\"\"This function returns n number of one hour ranges.\"\"\"\n",
    "    start_time1_list: list = list()\n",
    "    end_time1_list: list = list()\n",
    "    for i in range(0, n):\n",
    "        s1, s2 = random_date()\n",
    "        while s1 in start_time1_list:\n",
    "            s1, s2 = random_date()\n",
    "        start_time1_list.append(s1)\n",
    "        end_time1_list.append(s2)\n",
    "\n",
    "    sorted_start = sort_timestamps(start_time1_list)\n",
    "    sorted_end = sort_timestamps(end_time1_list)\n",
    "    return (sorted_start, sorted_end)\n",
    "\n",
    "\n",
    "def tweets_per_range(keyword: str, start_times_list: list, end_times_list: list, results_per_range: int, next_token: Optional[str] = None) -> None:\n",
    "    \"\"\"This function takes a keyword(s), a list of start times and end times, and a integer amount of results per range.\"\"\"\n",
    "    \"\"\"It then creates json files for each time range and stores the tweet results in those json files.\"\"\"\n",
    "    \n",
    "    # AUTHENTICATION\n",
    "    bearer_token = auth()\n",
    "    headers: dict[str, str] = create_headers(bearer_token)\n",
    "    # AUTHENTICATION\n",
    "\n",
    "    json_obj_by_time_range: dict[str, dict] = dict() # creates a dictionary to which the time range will be created as a key and can therefore be found while in the json file\n",
    "    max_results: int = 500 # this is the max results per request the twitter API allows and should be left at 500\n",
    "    total_tweets_for_func_call: int = 0\n",
    "\n",
    "    # Loops through the time ranges in a list\n",
    "    for i in range(0, len(start_times_list)):\n",
    "        json_obj_by_time_range: dict[str, dict] = dict()\n",
    "        print(\"top of FOR loop\")\n",
    "        total_count = 0 # Tracks\n",
    "        \n",
    "        # Creates url and connects to endpoint then assignts the json object API response to json_obj_response\n",
    "        url = create_full_search_url(keyword, start_times_list[i], end_times_list[i], max_results)\n",
    "        json_obj_response = connect_to_endpoint(url[0], headers, url[1], next_token) # prints response code\n",
    "        print(f\"Outer for loop enpoint called for list index {i} / {len(start_times_list) - 1}\") # for quality control\n",
    "        json_obj_response.pop('includes', None) # removes 'includes' key which is a negative externality of calling 'referenced_tweets.id.author_id' expansion\n",
    "        json_obj_response['time'] = [start_times_list[i], end_times_list[i]] # adds a 'time' key to the json_obj_response so that the time range of all tweets can be found by calling json_object_response['time']\n",
    "\n",
    "        # Appends the json object API response to the json_obj_by_time_range dictionary\n",
    "        json_obj_by_time_range[f'time_range_{i}'] = json_obj_response\n",
    "        total_count += json_obj_response['meta']['result_count'] # increases the total count counter by the results count in the first API 'GET'\n",
    "        print(f\"endpoint called and data collected: {total_count} / {results_per_range} tweets in this range scraped\")\n",
    "        time.sleep(5) # time.sleep implementations are seen throughout the code to avoid hitting rate limits of twitter API\n",
    "        \n",
    "        # If there were insifficient results from first 'GET' request to meet the results per range value then the API begins to paginate to scrape more resutls\n",
    "        while total_count <= results_per_range: \n",
    "            # Checks for next token in json response\n",
    "            print(\"top of WHILE loop\")\n",
    "            if 'next_token' in json_obj_response['meta']: \n",
    "                \n",
    "                next_token: str = json_obj_response['meta']['next_token'] # assigns 'next_token' to next_token: str object for easy use\n",
    "\n",
    "                # Creates url and connects to endpoint then assignts the JSON API response to json_obj_esponse\n",
    "                json_obj_response = connect_to_endpoint(url[0], headers, url[1], next_token) # prints response code\n",
    "                print(f\"While loop enpoint called: index {i} / {len(start_times_list) - 1}\") # for quality control\n",
    "                \n",
    "                next_token = None # ensures next token does not get passed into another function call \n",
    "\n",
    "                if 'data' in json_obj_response:\n",
    "                    # Loops through dictionaries in json_obj_response and appends them to the main json file\n",
    "                    # This is done because while theoretically the entire item count be appended at once, certain python vectorizing methods might cause disagreeable types. Looping through items avoids this happening\n",
    "                    for item in json_obj_response['data']:\n",
    "                        json_obj_by_time_range[f'time_range_{i}']['data'].append(item)\n",
    "                    \n",
    "                    total_count += json_obj_response['meta']['result_count'] # increments the result count to match the total results currently aquired\n",
    "                    json_obj_by_time_range[f'time_range_{i}']['meta']['result_count'] = total_count # changes the result count 'key' to meet the result count of all data\n",
    "\n",
    "                    if json_obj_by_time_range[f'time_range_{i}']['meta']['result_count'] > results_per_range:\n",
    "                        del json_obj_by_time_range[f'time_range_{i}']['data'][results_per_range:]\n",
    "                        json_obj_by_time_range[f'time_range_{i}']['meta']['result_count'] = len(json_obj_by_time_range[f'time_range_{i}']['data'])\n",
    "\n",
    "                    print(f\"data key found and data appended: {total_count} / {results_per_range} tweets in this range scraped\") # quality control\n",
    "                    print(f\"list index {i} / {(len(start_times_list) - 1)}\")\n",
    "\n",
    "                else:\n",
    "                    print(\"empty next token\") # quality control\n",
    "                    print(f\"max results scraped: {total_count} / {results_per_range} tweets in this range scraped\") # quality control\n",
    "                    print(f\"list index {i} / {(len(start_times_list) - 1)} scraping over. Total tweets will be less than desired\")\n",
    "                    break\n",
    "            \n",
    "            else:\n",
    "                print(\"No more tweets to scrape, total tweets will be less than amount desired.\") # quality control\n",
    "                print(f\"total results {total_count}\") # quality control\n",
    "                next_token = None # ensures next token does not get passed into another function call\n",
    "                break # exits while loop and calls for the next time range or terminates entire process\n",
    "            \n",
    "\n",
    "            time.sleep(5) # for rate limit \n",
    "        total_tweets_for_func_call += len(json_obj_by_time_range[f'time_range_{i}']['data'])\n",
    "        print(f\"{total_tweets_for_func_call} tweets scraped in entire function call\")\n",
    "        time.sleep(5) # for rate limit\n",
    "\n",
    "            \n",
    "        json_to_file = json.dumps(json_obj_by_time_range) # converts results from above code to a serialized json obj\n",
    "        # Creates json file in directory of program and writes serialized json obj to the newly made file\n",
    "        with open(f\"data_range_{i}.json\", \"w\") as outfile:\n",
    "            outfile.write(json_to_file)\n",
    "\n",
    "def tweets_per_range_replies_to_user(user: str, start_times_list: list, end_times_list: list, results_per_range: int, next_token: Optional[str] = None) -> None:\n",
    "    \"\"\"This function takes a keyword(s), a list of start times and end times, and a integer amount of results per range.\"\"\"\n",
    "    \"\"\"It then creates json files for each time range and stores the tweet results in those json files.\"\"\"\n",
    "    \n",
    "    # AUTHENTICATION\n",
    "    bearer_token = auth()\n",
    "    headers: dict[str, str] = create_headers(bearer_token)\n",
    "    # AUTHENTICATION\n",
    "\n",
    "    json_obj_by_time_range: dict[str, dict] = dict() # creates a dictionary to which the time range will be created as a key and can therefore be found while in the json file\n",
    "    max_results: int = 500 # this is the max results per request the twitter API allows and should be left at 500\n",
    "    total_tweets_for_func_call: int = 0\n",
    "\n",
    "    # Loops through the time ranges in a list\n",
    "    for i in range(0, len(start_times_list)):\n",
    "        json_obj_by_time_range: dict[str, dict] = dict()\n",
    "        print(\"top of FOR loop\")\n",
    "        total_count = 0 # Tracks\n",
    "        \n",
    "        # Creates url and connects to endpoint then assignts the json object API response to json_obj_response\n",
    "        url = create_full_search_url(keyword, start_times_list[i], end_times_list[i], max_results)\n",
    "        json_obj_response = connect_to_endpoint(url[0], headers, url[1], next_token) # prints response code\n",
    "        print(f\"Outer for loop enpoint called for list index {i} / {len(start_times_list) - 1}\") # for quality control\n",
    "        json_obj_response.pop('includes', None) # removes 'includes' key which is a negative externality of calling 'referenced_tweets.id.author_id' expansion\n",
    "        json_obj_response['time'] = [start_times_list[i], end_times_list[i]] # adds a 'time' key to the json_obj_response so that the time range of all tweets can be found by calling json_object_response['time']\n",
    "\n",
    "        # Appends the json object API response to the json_obj_by_time_range dictionary\n",
    "        json_obj_by_time_range[f'time_range_{i}'] = json_obj_response\n",
    "        total_count += json_obj_response['meta']['result_count'] # increases the total count counter by the results count in the first API 'GET'\n",
    "        print(f\"endpoint called and data collected: {total_count} / {results_per_range} tweets in this range scraped\")\n",
    "        time.sleep(5) # time.sleep implementations are seen throughout the code to avoid hitting rate limits of twitter API\n",
    "        \n",
    "        # If there were insifficient results from first 'GET' request to meet the results per range value then the API begins to paginate to scrape more resutls\n",
    "        while total_count <= results_per_range: \n",
    "            # Checks for next token in json response\n",
    "            print(\"top of WHILE loop\")\n",
    "            if 'next_token' in json_obj_response['meta']: \n",
    "                \n",
    "                next_token: str = json_obj_response['meta']['next_token'] # assigns 'next_token' to next_token: str object for easy use\n",
    "\n",
    "                # Creates url and connects to endpoint then assignts the JSON API response to json_obj_esponse\n",
    "                json_obj_response = connect_to_endpoint(url[0], headers, url[1], next_token) # prints response code\n",
    "                print(f\"While loop enpoint called: index {i} / {len(start_times_list) - 1}\") # for quality control\n",
    "                \n",
    "                next_token = None # ensures next token does not get passed into another function call \n",
    "\n",
    "                if 'data' in json_obj_response:\n",
    "                    # Loops through dictionaries in json_obj_response and appends them to the main json file\n",
    "                    # This is done because while theoretically the entire item count be appended at once, certain python vectorizing methods might cause disagreeable types. Looping through items avoids this happening\n",
    "                    for item in json_obj_response['data']:\n",
    "                        json_obj_by_time_range[f'time_range_{i}']['data'].append(item)\n",
    "                    \n",
    "                    total_count += json_obj_response['meta']['result_count'] # increments the result count to match the total results currently aquired\n",
    "                    json_obj_by_time_range[f'time_range_{i}']['meta']['result_count'] = total_count # changes the result count 'key' to meet the result count of all data\n",
    "\n",
    "                    if json_obj_by_time_range[f'time_range_{i}']['meta']['result_count'] > results_per_range:\n",
    "                        del json_obj_by_time_range[f'time_range_{i}']['data'][results_per_range:]\n",
    "                        json_obj_by_time_range[f'time_range_{i}']['meta']['result_count'] = len(json_obj_by_time_range[f'time_range_{i}']['data'])\n",
    "\n",
    "                    print(f\"data key found and data appended: {total_count} / {results_per_range} tweets in this range scraped\") # quality control\n",
    "                    print(f\"list index {i} / {(len(start_times_list) - 1)}\")\n",
    "\n",
    "                else:\n",
    "                    print(\"empty next token\") # quality control\n",
    "                    print(f\"max results scraped: {total_count} / {results_per_range} tweets in this range scraped\") # quality control\n",
    "                    print(f\"list index {i} / {(len(start_times_list) - 1)} scraping over. Total tweets will be less than desired\")\n",
    "                    break\n",
    "            \n",
    "            else:\n",
    "                print(\"No more tweets to scrape, total tweets will be less than amount desired.\") # quality control\n",
    "                print(f\"total results {total_count}\") # quality control\n",
    "                next_token = None # ensures next token does not get passed into another function call\n",
    "                break # exits while loop and calls for the next time range or terminates entire process\n",
    "            \n",
    "\n",
    "            time.sleep(5) # for rate limit \n",
    "        total_tweets_for_func_call += len(json_obj_by_time_range[f'time_range_{i}']['data'])\n",
    "        print(f\"{total_tweets_for_func_call} tweets scraped in entire function call\")\n",
    "        time.sleep(5) # for rate limit\n",
    "\n",
    "            \n",
    "        json_to_file = json.dumps(json_obj_by_time_range) # converts results from above code to a serialized json obj\n",
    "        # Creates json file in directory of program and writes serialized json obj to the newly made file\n",
    "        with open(f\"data_range_{i}.json\", \"w\") as outfile:\n",
    "            outfile.write(json_to_file)\n",
    "        \n",
    "\n",
    "\"\"\"This function takes all of the json files of tweet data and creates a sorted dictionary of the most appearing tweets.\"\"\"\n",
    "\"\"\"A post OR a retweet counts as ONE occurence of a tweet.\"\"\"\n",
    "def analyze_top_appearing_tweets_in_data(min_int_of_json: int, max_int_of_json: int, num_of_top_tweets: int) -> dict: # max_int_of_json is the max int of json files in your directory, this function will paginate through them\n",
    "    tweet_metrics_dict: dict = dict()\n",
    "    for i in range(0, max_int_of_json + 1):\n",
    "\n",
    "        # opens json file and assigns serialized json data to data_file \n",
    "        f = open(f'data_range_{i}.json')\n",
    "        data_file = json.load(f)\n",
    "\n",
    "        for item in data_file[f'time_range_{i}']['data']: # loops through the tweets in json file and adds to a dict key of that tweet's id if the tweet appears\n",
    "            temp_dict: dict = dict()\n",
    "            # Increments each tweet id 'key' in tweet_metrics_dict by one for each appearance of a tweet or each retweet of that tweet\n",
    "            if 'referenced_tweets' in item and item['referenced_tweets'][0]['type'] == \"retweeted\":\n",
    "                original_tweet_id_from_retweet = item['referenced_tweets'][0]['id']\n",
    "                if original_tweet_id_from_retweet in tweet_metrics_dict:\n",
    "                    tweet_metrics_dict[original_tweet_id_from_retweet]['appearance_count'] += 1\n",
    "            elif 'referenced_tweets' not in item:\n",
    "                temp_dict['appearance_count'] = 1\n",
    "                temp_dict['created_at'] = item['created_at']\n",
    "                tweet_metrics_dict[original_tweet_id_from_retweet] = temp_dict\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        f.close() # closes the json file\n",
    "\n",
    "    #sorted_dict = {} # creates a dictionary to assist with sorting tweet id's by the most appearances\n",
    "    #sorted_keys = sorted(tweet_metrics_dict, key=tweet_metrics_dict.get, reverse=True)  # sorts the keys by highes value (most tweet appearances)\n",
    "\n",
    "    #for w in sorted_keys:\n",
    "        #sorted_dict[w] = tweet_metrics_dict[w] # sorts the dictionary by value from greates to least\n",
    "    # Sort the dictionary by the \"total_appearances_in_dataset\" value in descending order\n",
    "    sorted_dict = dict(sorted(tweet_metrics_dict.items(), key=lambda x: x[1]['appearance_count'], reverse=True))\n",
    "    final_dict = delete_after_num_in_dict(num_of_top_tweets, sorted_dict)\n",
    "    return final_dict # returns sorted dict\n",
    "\n",
    "def analyze_top_appearing_tweets_in_data_wrong_demo(max_int_of_json: int) -> dict: # max_int_of_json is the max int of json files in your directory, this function will paginate through them\n",
    "    tweet_metrics_dict: dict = dict()\n",
    "    for i in range(0, max_int_of_json + 1):\n",
    "\n",
    "        # opens json file and assigns serialized json data to data_file \n",
    "        f = open(f'data_range_{i}.json')\n",
    "        data_file = json.load(f)\n",
    "\n",
    "        for item in data_file[f'time_range_{i}']['data']: # loops through the tweets in json file and adds to a dict key of that tweet's id if the tweet appears\n",
    "            # Increments each tweet id 'key' in tweet_metrics_dict by one for each appearance of a tweet or each retweet of that tweet\n",
    "            if 'referenced_tweets' in item and item['referenced_tweets'][0]['type'] == \"retweeted\":\n",
    "                original_tweet_id_from_retweet = item['referenced_tweets'][0]['id']\n",
    "                if original_tweet_id_from_retweet in tweet_metrics_dict:\n",
    "                    tweet_metrics_dict[original_tweet_id_from_retweet] += 1\n",
    "                else:\n",
    "                    tweet_metrics_dict[original_tweet_id_from_retweet] = 1\n",
    "            else:\n",
    "                this_tweet_id = item['id']\n",
    "                if this_tweet_id in tweet_metrics_dict:\n",
    "                    tweet_metrics_dict[this_tweet_id] += 1\n",
    "                else:\n",
    "                    tweet_metrics_dict[this_tweet_id] = 1\n",
    "\n",
    "        f.close() # closes the json file\n",
    "\n",
    "    sorted_dict = {} # creates a dictionary to assist with sorting tweet id's by the most appearances\n",
    "    sorted_keys = sorted(tweet_metrics_dict, key=tweet_metrics_dict.get, reverse=True)  # sorts the keys by highes value (most tweet appearances)\n",
    "\n",
    "    for w in sorted_keys:\n",
    "        sorted_dict[w] = tweet_metrics_dict[w] # sorts the dictionary by value from greates to least\n",
    "    final = delete_after_num_in_dict(1100, sorted_dict)\n",
    "    return final # returns sorted dict\n",
    "\n",
    "\n",
    "def return_stats_of_file(num_of_file: int):\n",
    "    f = open(f'data_range_{num_of_file}.json')\n",
    "    data_file = json.load(f)\n",
    "    print(f\"There are {len(data_file[f'time_range_{num_of_file}']['data'])} tweets in this data file\")\n",
    "    print(data_file[f'time_range_{num_of_file}']['meta'])\n",
    "    print(data_file[f'time_range_{num_of_file}']['time'])\n",
    "    f.close()\n",
    "\n",
    "def delete_after_num_in_dict(num: int, dictionary_obj: dict):\n",
    "    count = 0\n",
    "    empty_dict = dict()\n",
    "    for item in dictionary_obj:\n",
    "        count += 1\n",
    "        if count > num:\n",
    "            return empty_dict\n",
    "        empty_dict[item] = dictionary_obj[item]\n",
    "\n",
    "def analyze_top_retweeted_tweets_in_data(min_int_of_json: int, max_int_of_json: int, num_of_top_tweets: int) -> dict: # max_int_of_json is the max int of json files in your directory, this function will paginate through them\n",
    "    tweet_metrics_dict: dict = dict()\n",
    "    tweet_metrics_dict['data'] = list()\n",
    "    for i in range(min_int_of_json, max_int_of_json + 1):\n",
    "\n",
    "        # opens json file and assigns serialized json data to data_file \n",
    "        f = open(f'data_range_{i}.json')\n",
    "        data_file = json.load(f)\n",
    "\n",
    "        for item in data_file[f'time_range_{i}']['data']: # loops through the tweets in json file and adds retweet count to a dict key of that tweet's id \n",
    "            # Creates a key for each unique tweet id and assigns its retweet count to that key\n",
    "            if 'referenced_tweets' in item and item['referenced_tweets'][0]['type'] == \"retweeted\":\n",
    "                original_tweet_id_from_retweet = item['referenced_tweets'][0]['id']\n",
    "                if original_tweet_id_from_retweet not in tweet_metrics_dict:\n",
    "                    retweet_count = item['public_metrics']['retweet_count']\n",
    "                    created_time = item['created_at']\n",
    "                    tweet_metrics_dict[original_tweet_id_from_retweet] = [retweet_count, created_time]\n",
    "            else:\n",
    "                this_tweet_id = item['id']\n",
    "                if this_tweet_id not in tweet_metrics_dict:\n",
    "                    created_time = item['created_at']\n",
    "                    tweet_metrics_dict[this_tweet_id] = [item['public_metrics']['retweet_count'], created_time]\n",
    "\n",
    "        f.close() # closes the json file\n",
    "\n",
    "    sorted_dict = {} # creates a dictionary to assist with sorting tweet id's by the most appearances\n",
    "    sorted_keys = sorted(tweet_metrics_dict, key=tweet_metrics_dict.get, reverse=True)  # sorts the keys by highes value (most retweets)\n",
    "\n",
    "    for w in sorted_keys:\n",
    "        sorted_dict[w] = tweet_metrics_dict[w] # sorts the dictionary by value from greates to least\n",
    "    final_dict = delete_after_num_in_dict(num_of_top_tweets, sorted_dict)\n",
    "    \n",
    "    #temp = len(final_dict)\n",
    "    #count = 0\n",
    "    #for item in final_dict:\n",
    "        #print(f\"{count} / {temp} item parsed\")\n",
    "        #count += 1\n",
    "        #final_dict[item][1] = get_created_at_of_tweet_id(item)\n",
    "    return final_dict # returns sorted dict\n",
    "\n",
    "\n",
    "\"\"\"Gathers num_of_results number of replies to a tweet from conversation_id.\"\"\"\n",
    "def json_replies_to_tweet_from_conversation_id(conversation_id: str, num_of_results: int):\n",
    "\n",
    "    # Make a request to the search endpoint to retrieve all tweets in the conversation\n",
    "    bearer_token = auth()\n",
    "    headers: dict[str, str] = create_headers(bearer_token)\n",
    "    total_replies_scraped: int = 0\n",
    "\n",
    "    response: requests.models.Response = requests.get(f\"https://api.twitter.com/2/tweets/search/all?max_results=100&query=conversation_id:{conversation_id} is:reply&tweet.fields=author_id,in_reply_to_user_id,created_at,conversation_id\", headers={\"Authorization\": f\"Bearer {bearer_token}\"})\n",
    "\n",
    "    # Extract the tweets from the response\n",
    "    reply_tweets: json = response.json()\n",
    "    tweets_temp: json = reply_tweets\n",
    "    total_replies_scraped = reply_tweets['meta']['result_count']\n",
    "\n",
    "    # Checks if more replies need to be scraped and finds the next token if true\"\n",
    "    while total_replies_scraped < num_of_results:\n",
    "        if 'next_token' in tweets_temp['meta']:\n",
    "            next_token1: str = tweets_temp['meta']['next_token']\n",
    "            response: requests.models.Response = requests.get(f\"https://api.twitter.com/2/tweets/search/all?max_results=100&next_token={next_token1}&query=conversation_id:{conversation_id} is:reply&tweet.fields=author_id,in_reply_to_user_id,created_at,conversation_id\", headers={\"Authorization\": f\"Bearer {bearer_token}\"})\n",
    "            tweets_temp = response.json()\n",
    "            if 'data' in tweets_temp:\n",
    "                total_replies_scraped += tweets_temp['meta']['result_count']\n",
    "                for item in tweets_temp['data']:\n",
    "                    reply_tweets['data'].append(item)\n",
    "                reply_tweets['meta']['result_count'] = total_replies_scraped\n",
    "                if reply_tweets['meta']['result_count'] > num_of_results:\n",
    "                    del reply_tweets['data'][num_of_results:]\n",
    "                    reply_tweets['meta']['result_count'] = len(reply_tweets['data'])\n",
    "            else:\n",
    "                print(\"empty next token\") # quality control\n",
    "                print(f\"max results scraped: {total_replies_scraped} / {num_of_results}\") # quality control\n",
    "                break\n",
    "        else:\n",
    "            print(\"No more tweets to scrape, total tweets will be less than amount desired.\") # quality control\n",
    "            print(f\"total results {total_replies_scraped}\") # quality control\n",
    "            break\n",
    "        time.sleep(5) # for rate limit \n",
    "\n",
    "        # Print the tweets\n",
    "    print(f\"tweet replies scraped/replies desired: {total_replies_scraped} / {num_of_results}\") # quality control\n",
    "    print(json.dumps(reply_tweets, indent=5))\n",
    "\n",
    "def get_username_from_tweet_id(tweet_id: str):\n",
    "    # Define the URL of the tweet endpoint\n",
    "    tweet_url = f'https://api.twitter.com/2/users?ids={get_author_id_from_tweet_id(tweet_id)}'\n",
    "\n",
    "    # Define your Twitter API credentials\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {auth()}',\n",
    "        'User-Agent': 'YOUR_APP_NAME',\n",
    "    }\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(tweet_url, headers=headers)\n",
    "    print(response.status_code)\n",
    "    time.sleep(3.5)\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Load the response data into a Python dictionary\n",
    "        tweet = json.loads(response.text)\n",
    "        if 'data' in tweet:\n",
    "            return(tweet['data'][0]['username'])\n",
    "        else:\n",
    "            return(f\"\")\n",
    "    else:\n",
    "        return(f\"\")\n",
    "\n",
    "def collect_replies_to_user_from_username(username: str, start_time: str, end_time: str, num_replies: int):\n",
    "    \n",
    "    user_name: str = username\n",
    "    query = f'to%3A{user_name}'\n",
    "    #query = 'to%3Adfreelon'\n",
    "    #query = from%3Adfreelon\n",
    "    tweet_fields = 'created_at,entities,public_metrics,id,text,lang'\n",
    "    user_fields = 'created_at,description,entities,id,location,name,public_metrics,username,verified'\n",
    "    #start_time = \"2022-03-02T05:00:00Z\"\n",
    "    #end_time = \"2022-03-06T14:00:00Z\"\n",
    "    expansions = 'referenced_tweets.id.author_id'\n",
    "\n",
    "    headers1 = create_headers(auth())\n",
    "\n",
    "    url = 'https://api.twitter.com/2/tweets/search/all?query=' + \\\n",
    "                    query + \\\n",
    "                    '&max_results=500&start_time=' + \\\n",
    "                    start_time + \\\n",
    "                    '&end_time=' + \\\n",
    "                    end_time + \\\n",
    "                    '&tweet.fields=' + \\\n",
    "                    tweet_fields + \\\n",
    "                    '&expansions=' + \\\n",
    "                    expansions + \\\n",
    "                    '&user.fields=' + \\\n",
    "                    user_fields \n",
    "\n",
    "    # Creates url and connects to endpoint then assignts the json object API response to json_obj_response\n",
    "    #reply_url = create_full_search_url('to%3Adfreelon', start_time, end_time, 100)\n",
    "    response = requests.request(\"GET\", url, headers = headers1)\n",
    "    json_response = response.json()\n",
    "    if 'meta' not in json_response:\n",
    "        print(json_response)\n",
    "        return json_response\n",
    "    else:\n",
    "        total_count = json_response['meta']['result_count']\n",
    "\n",
    "    while total_count <= (num_replies - 100): \n",
    "            # Checks for next token in json response\n",
    "            print(\"top of WHILE loop\")\n",
    "            \n",
    "            if 'next_token' in json_response['meta']: \n",
    "                \n",
    "                if (num_replies - total_count) < 500:\n",
    "                    max_nums = (num_replies - total_count)\n",
    "                else:\n",
    "                    max_nums = 500\n",
    "                next_token: str = json_response['meta']['next_token'] # assigns 'next_token' to next_token: str object for easy use\n",
    "            \n",
    "                url = 'https://api.twitter.com/2/tweets/search/all?query=' + \\\n",
    "                    query + \\\n",
    "                    f'&max_results={max_nums}&start_time=' + \\\n",
    "                    start_time + \\\n",
    "                    '&end_time=' + \\\n",
    "                    end_time + \\\n",
    "                    '&tweet.fields=' + \\\n",
    "                    tweet_fields + \\\n",
    "                    '&expansions=' + \\\n",
    "                    expansions + \\\n",
    "                    '&user.fields=' + \\\n",
    "                    user_fields + \\\n",
    "                    '&next_token=' + \\\n",
    "                    next_token\n",
    "\n",
    "                # Creates url and connects to endpoint then assignts the JSON API response to json_obj_esponse\n",
    "                response = requests.request(\"GET\", url, headers = headers1) # prints response code\n",
    "                json_response2 = response.json()\n",
    "                #json_response['data'].append(json_response2)\n",
    "                print(f\"While loop enpoint called\") # for quality control\n",
    "                \n",
    "                next_token = None # ensures next token does not get passed into another function call \n",
    "\n",
    "                if 'data' in json_response2:\n",
    "                    # Loops through dictionaries in json_obj_response and appends them to the main json file\n",
    "                    # This is done because while theoretically the entire item count be appended at once, certain python vectorizing methods might cause disagreeable types. Looping through items avoids this happening\n",
    "                    for item in json_response2['data']:\n",
    "                        json_response['data'].append(item)\n",
    "                    if 'includes' in json_response2:\n",
    "                        if 'users' in json_response2['includes']:\n",
    "                            for item in json_response2['includes']['users']:\n",
    "                                json_response['includes']['users'].append(item)\n",
    "                        if 'tweets' in json_response2['includes']:\n",
    "                            for item in json_response2['includes']['tweets']:\n",
    "                                json_response['includes']['tweets'].append(item)\n",
    "                        \n",
    "\n",
    "                    total_count += json_response2['meta']['result_count'] # increments the result count to match the total results currently aquired\n",
    "                    json_response['meta']['result_count'] = total_count # changes the result count 'key' to meet the result count of all data\n",
    "                    if 'meta' in json_response2:\n",
    "                        if 'next_token' in json_response2['meta']:\n",
    "                            json_response['meta']['next_token'] = json_response2['meta']['next_token']\n",
    "                    else:\n",
    "                        json_response['meta']['next_token'] = None\n",
    "\n",
    "\n",
    "                    if json_response['meta']['result_count'] > num_replies:\n",
    "                        del json_response['data'][num_replies:]\n",
    "                        json_response['meta']['result_count'] = len(json_response['data'])\n",
    "\n",
    "                    print(f\"data key found and data appended: {total_count} / {num_replies} tweets in this range scraped\") # quality control\n",
    "                    json_response2 = {}\n",
    "                    \n",
    "                else:\n",
    "                    print(\"empty next token\") # quality control\n",
    "                    print(f\"max results scraped: {total_count} / {num_replies} tweets in this range scraped\") # quality control\n",
    "                    print(f\"Total tweets will be less than desired\")\n",
    "                    break\n",
    "            \n",
    "            else:\n",
    "                print(\"No more tweets to scrape, total tweets will be less than amount desired.\") # quality control\n",
    "                print(f\"total results {total_count}\") # quality control\n",
    "                next_token = None # ensures next token does not get passed into another function call\n",
    "                break # exits while loop and calls for the next time range or terminates entire process\n",
    "            \n",
    "\n",
    "            time.sleep(5) # for rate limit \n",
    "    print(total_count)\n",
    "    if 'data' in json_response:\n",
    "        total_count = len(json_response['data'])\n",
    "    print(f\"{total_count} tweets scraped in entire function call\")\n",
    "    time.sleep(5) # for rate limit\n",
    "\n",
    "    # converts results from above code to a serialized json obj\n",
    "    return(json_response)\n",
    "\n",
    "def get_created_at_of_tweet_id(tweet_id: str):\n",
    "    \"\"\"Creates queries to find the author id of a tweet's author.\"\"\"\n",
    "    search_url: str = f\"https://api.twitter.com/2/tweets/{tweet_id}\" \n",
    "\n",
    "    # change params to desired params\n",
    "    query_params: dict = {'tweet.fields': 'created_at'}\n",
    "    b_tok = auth()\n",
    "    headers = create_headers(b_tok)\n",
    "    response = requests.request(\"GET\", search_url, headers = headers, params = query_params)\n",
    "    time.sleep(4)\n",
    "    print(\"\\nEndpoint Response Code: \" + str(response.status_code)) # prints the enpoint response code for debugging help\n",
    "    json_response = response.json()\n",
    "    if 'data' in json_response:\n",
    "        if 'created_at' in json_response['data']:\n",
    "            return(json_response['data']['created_at'])\n",
    "        else:\n",
    "            return(tweet_id)\n",
    "    else:\n",
    "        return(tweet_id)\n",
    "    \n",
    "def add_week_to_timestamp(timestamp):\n",
    "    # parse the input timestamp into a datetime object\n",
    "    dt = datetime.datetime.fromisoformat(timestamp)\n",
    "\n",
    "    # add one week to the datetime object\n",
    "    dt = dt + datetime.timedelta(weeks=1)\n",
    "\n",
    "    # return the resulting timestamp in the desired format\n",
    "    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "def add_day_to_timestamp(timestamp):\n",
    "    # parse the input timestamp into a datetime object\n",
    "    dt = datetime.datetime.fromisoformat(timestamp)\n",
    "\n",
    "    # add one week to the datetime object\n",
    "    dt = dt + datetime.timedelta(days=1)\n",
    "\n",
    "    # return the resulting timestamp in the desired format\n",
    "    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "def creates_json_file_of_replies_to_top_users(dict_from_analyze_top_retweeted: dict):\n",
    "    json_replies_to_most_retweeted_users: json = dict()\n",
    "    count = 1\n",
    "    for item in dict_from_analyze_top_retweeted:\n",
    "        print(f\"{count} / {len(dict_from_analyze_top_retweeted)}\")\n",
    "        count += 1\n",
    "        username_of_tweet = dict_from_analyze_top_retweeted[item][2]\n",
    "        tweet_created_time = dict_from_analyze_top_retweeted[item][1]\n",
    "        one_week_after_tweet_created = add_week_to_timestamp(tweet_created_time)\n",
    "        response = collect_replies_to_user_from_username(username_of_tweet, tweet_created_time, one_week_after_tweet_created, 1000)\n",
    "        if 'meta' in response:\n",
    "            response['meta']['retweet_stats'] = (dict_from_analyze_top_retweeted[item])\n",
    "        json_replies_to_most_retweeted_users[username_of_tweet] = response\n",
    "    json_to_file = json.dumps(json_replies_to_most_retweeted_users)\n",
    "    with open('replies_to_100_most_retweeted_users.json', \"w\") as outfile:\n",
    "            outfile.write(json_to_file)\n",
    "\n",
    "def creates_json_file_of_replies_to_most_appearing_users(dict_from_analyze_top_retweeted: dict):\n",
    "    json_replies_to_most_appearing_users: json = dict()\n",
    "    count = 1\n",
    "    for item in dict_from_analyze_top_retweeted:\n",
    "        print(f\"{count} / {len(dict_from_analyze_top_retweeted)}\")\n",
    "        count += 1\n",
    "        username_of_tweet = dict_from_analyze_top_retweeted[item]['username']\n",
    "        tweet_created_time = dict_from_analyze_top_retweeted[item]['created_at']\n",
    "        one_week_after_tweet_created = add_week_to_timestamp(tweet_created_time)\n",
    "        response = collect_replies_to_user_from_username(username_of_tweet, tweet_created_time, one_week_after_tweet_created, 1000)\n",
    "        if 'meta' in response:\n",
    "            response['meta']['retweet_stats'] = (dict_from_analyze_top_retweeted[item])\n",
    "        json_replies_to_most_appearing_users[username_of_tweet] = response\n",
    "    json_to_file = json.dumps(json_replies_to_most_appearing_users)\n",
    "    with open('replies_to_100_most_appearing_users.json', \"w\") as outfile:\n",
    "            outfile.write(json_to_file)\n",
    "\n",
    "def creates_json_file_of_replies_to_non_seeder_users_over_year(dict_from_analyze_top_retweeted: dict):\n",
    "    over_annum_json_replies_to_most_appearing_users: json = dict()\n",
    "    count = 1\n",
    "    list_of_non_seeders: list = (\"Paragenio\", \"metarobox\", \"MeBoxBSC\", \"Bit2Me_Global\", \"SeedifyFund\", \"Stepnofficial\", \"TheCryptoLaunch\", \"MetaFi_Official\", \"Bull100X\", \"TopGoal_NFT\", )\n",
    "    for item in dict_from_analyze_top_retweeted:\n",
    "        if item in list_of_non_seeders:\n",
    "            count += 1\n",
    "            break\n",
    "        print(f\"{count} / {len(dict_from_analyze_top_retweeted)}\")\n",
    "        count += 1\n",
    "        username_of_tweet = dict_from_analyze_top_retweeted[item]['username']\n",
    "        \n",
    "        one_week_after_tweet_created = add_week_to_timestamp(tweet_created_time)\n",
    "        response = collect_replies_to_user_from_username(username_of_tweet, tweet_created_time, one_week_after_tweet_created, 1000)\n",
    "        if 'meta' in response:\n",
    "            response['meta']['retweet_stats'] = (dict_from_analyze_top_retweeted[item])\n",
    "        json_replies_to_most_appearing_users[username_of_tweet] = response\n",
    "    json_to_file = json.dumps(json_replies_to_most_appearing_users)\n",
    "    with open('replies_to_100_most_appearing_users.json', \"w\") as outfile:\n",
    "            outfile.write(json_to_file)\n",
    "\n",
    "def return_average_mentions_per_reply_to_top_retweeted_users(json_file: str):\n",
    "    f = open(json_file)\n",
    "    data_file = json.load(f)\n",
    "    usernames_avg_mentions = {}\n",
    "    for item in data_file:\n",
    "        #print(json.dumps(item,indent=5))\n",
    "        if 'data' in data_file[item]:\n",
    "            totals: list = list()\n",
    "            for tweet in data_file[item]['data']:\n",
    "                if ('entities' in tweet) and ('mentions' in tweet['entities']):\n",
    "                    totals.append(len(tweet['entities']['mentions']) - 1)\n",
    "                #usernames_avg_mentions[username] = mean(totals)\n",
    "            usernames_avg_mentions[item] = round(mean(totals), 5)\n",
    "            #print(f\"{item} has avg mentions per reply of {round(mean(totals), 5)}\")\n",
    "        else:\n",
    "            None\n",
    "            #print(f\"{item} is empty\")\n",
    "    sorted_dict = {} # creates a dictionary to assist with sorting tweet id's by the most appearances\n",
    "    sorted_keys = sorted(usernames_avg_mentions, key=usernames_avg_mentions.get, reverse=True)  # sorts the keys by highes value (most retweets)\n",
    "\n",
    "    for w in sorted_keys:\n",
    "        sorted_dict[w] = usernames_avg_mentions[w] # sorts the dictionary by value from greates to least\n",
    "    f.close()\n",
    "    return sorted_dict\n",
    "    \n",
    "        #usernames_avg_mentions[username] = mean(totals)\n",
    "\n",
    "def only_special_words(string):\n",
    "    for word in string.split():\n",
    "        if not word.startswith((\"@\")):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def return_users_with_at_least_three_mentions(json_file: str):\n",
    "    f = open(json_file)\n",
    "    data_file = json.load(f)\n",
    "    usernames_avg_mentions = {}\n",
    "    for item in data_file:\n",
    "        count = 0\n",
    "        #print(json.dumps(item,indent=5))\n",
    "        if 'data' in data_file[item]:\n",
    "            totals: list = list()\n",
    "            for tweet in data_file[item]['data']:\n",
    "                if ('entities' in tweet) and ('mentions' in tweet['entities']):\n",
    "                    if ((len(tweet['entities']['mentions']) - 1) >= 3 and only_special_words(tweet['text'])):\n",
    "                        count += 1\n",
    "            #print(f\"{item}: {count}\")\n",
    "            usernames_avg_mentions[item] = ((count / len(data_file[item]['data'])) * 100)\n",
    "            #print(f\"{item} has avg mentions per reply of {round(mean(totals), 5)}\")\n",
    "        else:\n",
    "            None\n",
    "    sorted_dict = {} # creates a dictionary to assist with sorting tweet id's by the most appearances\n",
    "    sorted_keys = sorted(usernames_avg_mentions, key=usernames_avg_mentions.get, reverse=True)  # sorts the keys by highes value (most retweets)\n",
    "\n",
    "    for w in sorted_keys:\n",
    "        sorted_dict[w] = usernames_avg_mentions[w] # sorts the dictionary by value from greatest to least\n",
    "    f.close()\n",
    "    return sorted_dict\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Working Cell\n",
    "2 cells below:  \n",
    "\n",
    "Cell 1: collects 10k tweets per time range from 100 random hour ranges over the last year.  \n",
    "The cell then collects the top 105 retweeted tweets from the data collected (if a tweet is a retweet it finds the id and retweet count of the original tweet id) and collects 1000 replies to those users (replies are from the created_at time of the tweet, and up to a week after the created_at time; if the tweet was a retweet, then the replies are from created_at time of that retweet but replies to the original user). \n",
    "\n",
    "Cell 2: does the same thing as cell one except it does not rank tweets by retweet counts, but rather by appearances in the original dataset of 1million tweets. It simply parses through the data and counts original tweets appearances or retweets of an original tweet as an appearance for that tweet. Replies and other forms of 'referenced_tweets' are not counted. A tweet is only counted/incremented if it is an original tweet or the dataset has a retweet of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This is and example function use which retrieves 10,000 tweets per hour range for 100 random hour ranges from the past year.\"\"\"\n",
    "start_list, end_list = return_n_random_hour_ranges_sorted(100)\n",
    "json_final_data = tweets_per_range(\"Smartchain OR Airdrop OR Crypto OR Nft\", start_list, end_list, 10000)\n",
    "\n",
    "\"\"\"This function use returns the most retweeted tweets between the hour ranges desired.\"\"\"\n",
    "top_retweeted_tweet_ids_sorted = analyze_top_retweeted_tweets_in_data(0, 99, 105)\n",
    "\n",
    "\n",
    "# This section of code loops through the most retweeted tweet ids and collects the username of the authors of the tweets\n",
    "# --- Section start ---\n",
    "counter = 0\n",
    "for item in top_retweeted_tweet_ids_sorted:\n",
    "    top_retweeted_tweet_ids_sorted[item].append(get_username_from_tweet_id(item))\n",
    "    counter += 1\n",
    "    print(f\"{counter} / {len(top_retweeted_tweet_ids_sorted)}\")\n",
    "# --- Section end ---\n",
    " \n",
    "# This section writes the tweet ids and usernames to a dictionary for storage\n",
    "# --- Section start ---\n",
    "json_2_file = json.dumps(top_retweeted_tweet_ids_sorted)\n",
    "with open(f\"top_105_retweeted_users.json\", \"w\") as outfile:\n",
    "    outfile.write(json_2_file)\n",
    "# --- Section end ---\n",
    "\n",
    "f = open('top_105_retweeted_users.json')\n",
    "data_file = json.load(f)\n",
    "f.close()\n",
    "creates_json_file_of_replies_to_top_users(data_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This is and example function use which retrieves 10,000 tweets per hour range for 100 random hour ranges from the past year.\"\"\"\n",
    "start_list, end_list = return_n_random_hour_ranges_sorted(100)\n",
    "json_final_data = tweets_per_range(\"Smartchain OR Airdrop OR Crypto OR Nft\", start_list, end_list, 10000)\n",
    "\n",
    "\"\"\"This function use returns the most retweeted tweets between the hour ranges desired.\"\"\"\n",
    "top_retweeted_tweet_ids_sorted = analyze_top_retweeted_tweets_in_data(0, 99, 105)\n",
    "\n",
    "top_appearing_tweet_ids = analyze_top_appearing_tweets_in_data(0, 99, 105)\n",
    "counter = 0\n",
    "for item in top_appearing_tweet_ids:\n",
    "    top_appearing_tweet_ids[item]['username'] = get_username_from_tweet_id(item)\n",
    "    counter += 1\n",
    "    print(f\"{counter} / {len(top_appearing_tweet_ids)}\")\n",
    "# --- Section end ---\n",
    " \n",
    "# This section writes the tweet ids and usernames to a dictionary for storage\n",
    "# --- Section start ---\n",
    "json_2_file = json.dumps(top_appearing_tweet_ids)\n",
    "with open(f\"most_100_appearing_tweet_ids.json\", \"w\") as outfile:\n",
    "    outfile.write(json_2_file)\n",
    "# --- Section end ---\n",
    "f = open('most_100_appearing_tweet_ids.json')\n",
    "data_file = json.load(f)\n",
    "f.close()\n",
    "creates_json_file_of_replies_to_most_appearing_users(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.35365853658537% of apsocoin replies have at least three mentions AND only contain words starting with '@'\n",
      "38.69610935856993% of REVDReverse replies have at least three mentions AND only contain words starting with '@'\n",
      "37.13980789754536% of AirdropDaily_ replies have at least three mentions AND only contain words starting with '@'\n",
      "35.42976939203354% of ton_blockchain replies have at least three mentions AND only contain words starting with '@'\n",
      "34.21686746987952% of DesmoLabs replies have at least three mentions AND only contain words starting with '@'\n",
      "33.263816475495304% of DriveCityNFT replies have at least three mentions AND only contain words starting with '@'\n",
      "32.698094282848544% of StarbankFinance replies have at least three mentions AND only contain words starting with '@'\n",
      "31.14224137931034% of playermons replies have at least three mentions AND only contain words starting with '@'\n",
      "30.27426160337553% of AvaultOmni replies have at least three mentions AND only contain words starting with '@'\n",
      "29.295154185022028% of TheCryptoLaunch replies have at least three mentions AND only contain words starting with '@'\n",
      "21.910695742471443% of metarobox replies have at least three mentions AND only contain words starting with '@'\n",
      "21.62458836443469% of PinkPeaFinance replies have at least three mentions AND only contain words starting with '@'\n",
      "17.671517671517673% of MeBoxBSC replies have at least three mentions AND only contain words starting with '@'\n",
      "15.789473684210526% of Paragenio replies have at least three mentions AND only contain words starting with '@'\n",
      "15.464994775339601% of Bull100X replies have at least three mentions AND only contain words starting with '@'\n",
      "10.662525879917183% of METASPIDERMAN_I replies have at least three mentions AND only contain words starting with '@'\n",
      "10.080645161290322% of metafarmverse replies have at least three mentions AND only contain words starting with '@'\n",
      "9.547738693467336% of bitgertbrise replies have at least three mentions AND only contain words starting with '@'\n",
      "9.432048681541582% of RMoonOfficial replies have at least three mentions AND only contain words starting with '@'\n",
      "8.996897621509824% of DooDooToken replies have at least three mentions AND only contain words starting with '@'\n",
      "8.836206896551724% of NEST_Protocol replies have at least three mentions AND only contain words starting with '@'\n",
      "8.762886597938143% of DOGB_Token replies have at least three mentions AND only contain words starting with '@'\n",
      "8.682008368200837% of Hyundai_NFT replies have at least three mentions AND only contain words starting with '@'\n",
      "8.056394763343404% of TopGoal_NFT replies have at least three mentions AND only contain words starting with '@'\n",
      "8.047210300429185% of efuturecoin replies have at least three mentions AND only contain words starting with '@'\n",
      "7.822410147991543% of AirdropBSC_Com replies have at least three mentions AND only contain words starting with '@'\n",
      "7.739938080495357% of Airdrop365Team replies have at least three mentions AND only contain words starting with '@'\n",
      "7.510288065843622% of SoccerFan_io replies have at least three mentions AND only contain words starting with '@'\n",
      "7.240293809024134% of RdogToken replies have at least three mentions AND only contain words starting with '@'\n",
      "6.596858638743456% of Move___Official replies have at least three mentions AND only contain words starting with '@'\n",
      "6.329113924050633% of NEOKOREA_COIN replies have at least three mentions AND only contain words starting with '@'\n",
      "6.130653266331659% of LaeebTokenBSC replies have at least three mentions AND only contain words starting with '@'\n",
      "5.274261603375527% of jameszhao2023 replies have at least three mentions AND only contain words starting with '@'\n",
      "5.183585313174946% of Runlixclub replies have at least three mentions AND only contain words starting with '@'\n",
      "5.019305019305019% of ARBIDname replies have at least three mentions AND only contain words starting with '@'\n",
      "4.938271604938271% of Unlockd_Finance replies have at least three mentions AND only contain words starting with '@'\n",
      "4.856512141280353% of odosprotocol replies have at least three mentions AND only contain words starting with '@'\n",
      "4.6943231441048034% of MetaFi_Official replies have at least three mentions AND only contain words starting with '@'\n",
      "4.6875% of staika_official replies have at least three mentions AND only contain words starting with '@'\n",
      "4.2% of NFT3com replies have at least three mentions AND only contain words starting with '@'\n",
      "3.884572697003329% of duckienft replies have at least three mentions AND only contain words starting with '@'\n",
      "3.8335158817086525% of HeroArena_Hera replies have at least three mentions AND only contain words starting with '@'\n",
      "3.7871033776867966% of BlockChain_CK replies have at least three mentions AND only contain words starting with '@'\n",
      "3.571428571428571% of Bit2Me_Global replies have at least three mentions AND only contain words starting with '@'\n",
      "3.3653846153846154% of metarare_nft replies have at least three mentions AND only contain words starting with '@'\n",
      "3.3232628398791544% of Puke2Earn replies have at least three mentions AND only contain words starting with '@'\n",
      "3.1536113936927768% of TankMetaverse replies have at least three mentions AND only contain words starting with '@'\n",
      "2.8047464940668827% of FrankLinYield replies have at least three mentions AND only contain words starting with '@'\n",
      "2.547770700636943% of coinzixcom replies have at least three mentions AND only contain words starting with '@'\n",
      "2.454642475987193% of PlayCryptoBallZ replies have at least three mentions AND only contain words starting with '@'\n",
      "2.3952095808383236% of walken_io replies have at least three mentions AND only contain words starting with '@'\n",
      "2.2022022022022023% of ArivaCoin replies have at least three mentions AND only contain words starting with '@'\n",
      "2.181818181818182% of ApolloX_Finance replies have at least three mentions AND only contain words starting with '@'\n",
      "1.9387755102040816% of MN_BRIDGE replies have at least three mentions AND only contain words starting with '@'\n",
      "1.9114688128772637% of SpaceIDProtocol replies have at least three mentions AND only contain words starting with '@'\n",
      "1.8848167539267016% of AboardExchange replies have at least three mentions AND only contain words starting with '@'\n",
      "1.8108651911468814% of SeedifyFund replies have at least three mentions AND only contain words starting with '@'\n",
      "1.6597510373443984% of Stepnofficial replies have at least three mentions AND only contain words starting with '@'\n",
      "1.6064257028112447% of MyrtleGail replies have at least three mentions AND only contain words starting with '@'\n",
      "1.5151515151515151% of Helio_Money replies have at least three mentions AND only contain words starting with '@'\n",
      "1.503006012024048% of WizardLandSui replies have at least three mentions AND only contain words starting with '@'\n",
      "1.4098690835850958% of layer3xyz replies have at least three mentions AND only contain words starting with '@'\n",
      "1.402805611222445% of 1shootgame replies have at least three mentions AND only contain words starting with '@'\n",
      "1.3377926421404682% of X_Starter_ replies have at least three mentions AND only contain words starting with '@'\n",
      "1.310483870967742% of DRx_DeFi replies have at least three mentions AND only contain words starting with '@'\n",
      "1.3052208835341366% of STRMNFT replies have at least three mentions AND only contain words starting with '@'\n",
      "1.2618296529968454% of SNSstork replies have at least three mentions AND only contain words starting with '@'\n",
      "1.1235955056179776% of Mises001 replies have at least three mentions AND only contain words starting with '@'\n",
      "1.0999999999999999% of clashrow replies have at least three mentions AND only contain words starting with '@'\n",
      "1.0416666666666665% of nft_souffl3 replies have at least three mentions AND only contain words starting with '@'\n",
      "0.9814612868047983% of GalaxyBlitzGame replies have at least three mentions AND only contain words starting with '@'\n",
      "0.9803921568627451% of RoboWolf8 replies have at least three mentions AND only contain words starting with '@'\n",
      "0.9554140127388535% of VenusProtocol replies have at least three mentions AND only contain words starting with '@'\n",
      "0.9109311740890688% of ParaSpace_NFT replies have at least three mentions AND only contain words starting with '@'\n",
      "0.7608695652173914% of Tigon_Mobile replies have at least three mentions AND only contain words starting with '@'\n",
      "0.7021063189568706% of LFGSwap replies have at least three mentions AND only contain words starting with '@'\n",
      "0.6243496357960457% of SleepGame_ replies have at least three mentions AND only contain words starting with '@'\n",
      "0.41067761806981523% of zecreyprotocol replies have at least three mentions AND only contain words starting with '@'\n",
      "0.30272452068617556% of MaisonValentino replies have at least three mentions AND only contain words starting with '@'\n",
      "0.20597322348094746% of TheDaoMaker replies have at least three mentions AND only contain words starting with '@'\n",
      "0.2026342451874367% of binance replies have at least three mentions AND only contain words starting with '@'\n",
      "0.0% of elonmusk replies have at least three mentions AND only contain words starting with '@'\n",
      "0.0% of McDonalds replies have at least three mentions AND only contain words starting with '@'\n",
      "0.0% of nuacknuack replies have at least three mentions AND only contain words starting with '@'\n",
      "0.0% of Nillusion replies have at least three mentions AND only contain words starting with '@'\n",
      "0.0% of Monsta_Infinite replies have at least three mentions AND only contain words starting with '@'\n",
      "0.0% of Hare_Token replies have at least three mentions AND only contain words starting with '@'\n",
      "0.0% of koji_nu_nu replies have at least three mentions AND only contain words starting with '@'\n",
      "0.0% of fionasnapples replies have at least three mentions AND only contain words starting with '@'\n",
      "0.0% of StormFourGods replies have at least three mentions AND only contain words starting with '@'\n",
      "0.0% of asahiinryo_jp replies have at least three mentions AND only contain words starting with '@'\n",
      "0.0% of NaFun replies have at least three mentions AND only contain words starting with '@'\n",
      "0.0% of Shopify replies have at least three mentions AND only contain words starting with '@'\n"
     ]
    }
   ],
   "source": [
    "dict2 = return_users_with_at_least_three_mentions('replies_to_100_most_retweeted_users_good.json')\n",
    "for item in dict2:\n",
    "    print(f\"{dict2[item]}% of {item} replies have at least three mentions AND only contain words starting with '@'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "     \"1509447288409702401\": {\n",
      "          \"parent_user\": \"RdogToken\",\n",
      "          \"parent_id\": \"1499397639884132360\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1499397639884132360\"\n",
      "     },\n",
      "     \"1501163651172081671\": {\n",
      "          \"parent_user\": \"MeBoxBSC\",\n",
      "          \"parent_id\": \"1493848113232367618\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1493848113232367618\"\n",
      "     },\n",
      "     \"1501388097682092038\": {\n",
      "          \"parent_user\": \"metarobox\",\n",
      "          \"parent_id\": \"1495556777399955456\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1495556777399955456\"\n",
      "     },\n",
      "     \"1512472589372907523\": {\n",
      "          \"parent_user\": \"Unlockd_Finance\",\n",
      "          \"parent_id\": \"1511662470645043200\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1511662470645043200\"\n",
      "     },\n",
      "     \"1501435555153915912\": {\n",
      "          \"parent_user\": \"DriveCityNFT\",\n",
      "          \"parent_id\": \"1498573316314914818\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1498573316314914818\"\n",
      "     },\n",
      "     \"1507610688264581128\": {\n",
      "          \"parent_user\": \"walken_io\",\n",
      "          \"parent_id\": \"1491067405506068480\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1491067405506068480\"\n",
      "     },\n",
      "     \"1507403675559501834\": {\n",
      "          \"parent_user\": \"TheCryptoLaunch\",\n",
      "          \"parent_id\": \"1502680563622330372\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1502680563622330372\"\n",
      "     },\n",
      "     \"1507026644909051906\": {\n",
      "          \"parent_user\": \"REVDReverse\",\n",
      "          \"parent_id\": \"1500402939059339267\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1500402939059339267\"\n",
      "     },\n",
      "     \"1501291131237126150\": {\n",
      "          \"parent_user\": \"MeBoxBSC\",\n",
      "          \"parent_id\": \"1494612640047403008\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1494612640047403008\"\n",
      "     },\n",
      "     \"1501090127459741698\": {\n",
      "          \"parent_user\": \"metarobox\",\n",
      "          \"parent_id\": \"1495556777399955456\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1495556777399955456\"\n",
      "     },\n",
      "     \"1501264096804712455\": {\n",
      "          \"parent_user\": \"Bull100X\",\n",
      "          \"parent_id\": \"1491605958971191298\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1491605958971191298\"\n",
      "     },\n",
      "     \"1499779992448028674\": {\n",
      "          \"parent_user\": \"playermons\",\n",
      "          \"parent_id\": \"1496757407963181057\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1496757407963181057\"\n",
      "     },\n",
      "     \"1499098839432962048\": {\n",
      "          \"parent_user\": \"TopGoal_NFT\",\n",
      "          \"parent_id\": \"1469322247219281920\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1469322247219281920\"\n",
      "     },\n",
      "     \"1501298969363767298\": {\n",
      "          \"parent_user\": \"DriveCityNFT\",\n",
      "          \"parent_id\": \"1498573316314914818\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1498573316314914818\"\n",
      "     },\n",
      "     \"1500097959400140804\": {\n",
      "          \"parent_user\": \"playermons\",\n",
      "          \"parent_id\": \"1496757407963181057\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1496757407963181057\"\n",
      "     },\n",
      "     \"1506996017211846659\": {\n",
      "          \"parent_user\": \"REVDReverse\",\n",
      "          \"parent_id\": \"1500402939059339267\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1500402939059339267\"\n",
      "     },\n",
      "     \"1568143792452304897\": {\n",
      "          \"parent_user\": \"FrankLinYield\",\n",
      "          \"parent_id\": \"1567098760387493889\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1567098760387493889\"\n",
      "     },\n",
      "     \"1501272976699936768\": {\n",
      "          \"parent_user\": \"DriveCityNFT\",\n",
      "          \"parent_id\": \"1498573316314914818\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1498573316314914818\"\n",
      "     },\n",
      "     \"1541991612330348544\": {\n",
      "          \"parent_user\": \"DesmoLabs\",\n",
      "          \"parent_id\": \"1536291068840493060\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1536291068840493060\"\n",
      "     },\n",
      "     \"1507417415230976003\": {\n",
      "          \"parent_user\": \"TheCryptoLaunch\",\n",
      "          \"parent_id\": \"1502680563622330372\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1502680563622330372\"\n",
      "     },\n",
      "     \"1499476134488162318\": {\n",
      "          \"parent_user\": \"Bit2Me_Global\",\n",
      "          \"parent_id\": \"1491396137999163393\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1491396137999163393\"\n",
      "     },\n",
      "     \"1511677912155525124\": {\n",
      "          \"parent_user\": \"StarbankFinance\",\n",
      "          \"parent_id\": \"1509192744119730179\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1509192744119730179\"\n",
      "     },\n",
      "     \"1518988407657086979\": {\n",
      "          \"parent_user\": \"DOGB_Token\",\n",
      "          \"parent_id\": \"1515684055773294593\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1515684055773294593\"\n",
      "     },\n",
      "     \"1585324793091325952\": {\n",
      "          \"parent_user\": \"apsocoin\",\n",
      "          \"parent_id\": \"1583020880065888256\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1583020880065888256\"\n",
      "     },\n",
      "     \"1558746809551429632\": {\n",
      "          \"parent_user\": \"SeedifyFund\",\n",
      "          \"parent_id\": \"1558742338410782720\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1558742338410782720\"\n",
      "     },\n",
      "     \"1507034921826926592\": {\n",
      "          \"parent_user\": \"TheCryptoLaunch\",\n",
      "          \"parent_id\": \"1502680563622330372\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1502680563622330372\"\n",
      "     },\n",
      "     \"1526255746836357120\": {\n",
      "          \"parent_user\": \"Stepnofficial\",\n",
      "          \"parent_id\": \"1526213807110070272\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1526213807110070272\"\n",
      "     },\n",
      "     \"1507254380130619418\": {\n",
      "          \"parent_user\": \"TheCryptoLaunch\",\n",
      "          \"parent_id\": \"1392897877882507265\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1392897877882507265\"\n",
      "     },\n",
      "     \"1585943980004577280\": {\n",
      "          \"parent_user\": \"apsocoin\",\n",
      "          \"parent_id\": \"1583020880065888256\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1583020880065888256\"\n",
      "     },\n",
      "     \"1512157988915904512\": {\n",
      "          \"parent_user\": \"metafarmverse\",\n",
      "          \"parent_id\": \"1495338551759839235\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1495338551759839235\"\n",
      "     },\n",
      "     \"1542715166285918210\": {\n",
      "          \"parent_user\": \"DesmoLabs\",\n",
      "          \"parent_id\": \"1536291068840493060\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1536291068840493060\"\n",
      "     },\n",
      "     \"1509512966516854784\": {\n",
      "          \"parent_user\": \"AirdropDaily_\",\n",
      "          \"parent_id\": \"1505397360087961604\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1505397360087961604\"\n",
      "     },\n",
      "     \"1507257998921826304\": {\n",
      "          \"parent_user\": \"REVDReverse\",\n",
      "          \"parent_id\": \"1500402939059339267\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1500402939059339267\"\n",
      "     },\n",
      "     \"1507199370801954821\": {\n",
      "          \"parent_user\": \"TheCryptoLaunch\",\n",
      "          \"parent_id\": \"1502680563622330372\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1502680563622330372\"\n",
      "     },\n",
      "     \"1498928904748531714\": {\n",
      "          \"parent_user\": \"ton_blockchain\",\n",
      "          \"parent_id\": \"1496456914057715714\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1496456914057715714\"\n",
      "     },\n",
      "     \"1501122832260116483\": {\n",
      "          \"parent_user\": \"ton_blockchain\",\n",
      "          \"parent_id\": \"1496456914057715714\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1496456914057715714\"\n",
      "     },\n",
      "     \"1515939743447633920\": {\n",
      "          \"parent_user\": \"MetaFi_Official\",\n",
      "          \"parent_id\": \"1507992838340227072\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1507992838340227072\"\n",
      "     },\n",
      "     \"1519641515542339586\": {\n",
      "          \"parent_user\": \"Runlixclub\",\n",
      "          \"parent_id\": \"1518227583606788096\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1518227583606788096\"\n",
      "     },\n",
      "     \"1514583720988192774\": {\n",
      "          \"parent_user\": \"PlayCryptoBallZ\",\n",
      "          \"parent_id\": \"1508981405229129728\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1508981405229129728\"\n",
      "     },\n",
      "     \"1507026938996862978\": {\n",
      "          \"parent_user\": \"REVDReverse\",\n",
      "          \"parent_id\": \"1500402939059339267\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1500402939059339267\"\n",
      "     },\n",
      "     \"1501092977380184067\": {\n",
      "          \"parent_user\": \"playermons\",\n",
      "          \"parent_id\": \"1496757407963181057\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1496757407963181057\"\n",
      "     },\n",
      "     \"1511430577525858308\": {\n",
      "          \"parent_user\": \"AvaultOmni\",\n",
      "          \"parent_id\": \"1507311786324033550\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1507311786324033550\"\n",
      "     },\n",
      "     \"1518142831650607105\": {\n",
      "          \"parent_user\": \"NEST_Protocol\",\n",
      "          \"parent_id\": \"1516424499473879048\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1516424499473879048\"\n",
      "     },\n",
      "     \"1540879597851770880\": {\n",
      "          \"parent_user\": \"DesmoLabs\",\n",
      "          \"parent_id\": \"1536291068840493060\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1536291068840493060\"\n",
      "     },\n",
      "     \"1586519386642214912\": {\n",
      "          \"parent_user\": \"apsocoin\",\n",
      "          \"parent_id\": \"1583020880065888256\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1583020880065888256\"\n",
      "     },\n",
      "     \"1501166332611481627\": {\n",
      "          \"parent_user\": \"MeBoxBSC\",\n",
      "          \"parent_id\": \"1494612640047403008\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1494612640047403008\"\n",
      "     },\n",
      "     \"1586453138625933312\": {\n",
      "          \"parent_user\": \"apsocoin\",\n",
      "          \"parent_id\": \"1583020880065888256\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1583020880065888256\"\n",
      "     },\n",
      "     \"1509478853311614982\": {\n",
      "          \"parent_user\": \"AirdropDaily_\",\n",
      "          \"parent_id\": \"1505397360087961604\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1505397360087961604\"\n",
      "     },\n",
      "     \"1509518730266558469\": {\n",
      "          \"parent_user\": \"AirdropDaily_\",\n",
      "          \"parent_id\": \"1505397360087961604\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1505397360087961604\"\n",
      "     },\n",
      "     \"1501403088107802629\": {\n",
      "          \"parent_user\": \"MeBoxBSC\",\n",
      "          \"parent_id\": \"1493848113232367618\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1493848113232367618\"\n",
      "     },\n",
      "     \"1518571910426157056\": {\n",
      "          \"parent_user\": \"NEST_Protocol\",\n",
      "          \"parent_id\": \"1516424499473879048\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1516424499473879048\"\n",
      "     },\n",
      "     \"1540879754701963264\": {\n",
      "          \"parent_user\": \"DesmoLabs\",\n",
      "          \"parent_id\": \"1536291068840493060\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1536291068840493060\"\n",
      "     },\n",
      "     \"1501333402003202048\": {\n",
      "          \"parent_user\": \"DriveCityNFT\",\n",
      "          \"parent_id\": \"1498573316314914818\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1498573316314914818\"\n",
      "     },\n",
      "     \"1585878266702725120\": {\n",
      "          \"parent_user\": \"NEOKOREA_COIN\",\n",
      "          \"parent_id\": \"1583400205327953927\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1583400205327953927\"\n",
      "     },\n",
      "     \"1501184290998132736\": {\n",
      "          \"parent_user\": \"DriveCityNFT\",\n",
      "          \"parent_id\": \"1498573316314914818\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1498573316314914818\"\n",
      "     },\n",
      "     \"1509524873466449920\": {\n",
      "          \"parent_user\": \"AirdropDaily_\",\n",
      "          \"parent_id\": \"1502980257275875330\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1502980257275875330\"\n",
      "     },\n",
      "     \"1511590829298962432\": {\n",
      "          \"parent_user\": \"AvaultOmni\",\n",
      "          \"parent_id\": \"1507311786324033550\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1507311786324033550\"\n",
      "     },\n",
      "     \"1499657719149641728\": {\n",
      "          \"parent_user\": \"playermons\",\n",
      "          \"parent_id\": \"1496757407963181057\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1496757407963181057\"\n",
      "     },\n",
      "     \"1512329950392725504\": {\n",
      "          \"parent_user\": \"RMoonOfficial\",\n",
      "          \"parent_id\": \"1507273391354814465\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1507273391354814465\"\n",
      "     },\n",
      "     \"1514876888183095296\": {\n",
      "          \"parent_user\": \"PinkPeaFinance\",\n",
      "          \"parent_id\": \"1513392154437369856\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1513392154437369856\"\n",
      "     },\n",
      "     \"1499246104688611328\": {\n",
      "          \"parent_user\": \"ton_blockchain\",\n",
      "          \"parent_id\": \"1496456914057715714\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1496456914057715714\"\n",
      "     },\n",
      "     \"1584341612834230272\": {\n",
      "          \"parent_user\": \"NEOKOREA_COIN\",\n",
      "          \"parent_id\": \"1583400205327953927\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1583400205327953927\"\n",
      "     },\n",
      "     \"1511460751805984769\": {\n",
      "          \"parent_user\": \"AvaultOmni\",\n",
      "          \"parent_id\": \"1507311786324033550\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1507311786324033550\"\n",
      "     },\n",
      "     \"1520080938520444928\": {\n",
      "          \"parent_user\": \"Runlixclub\",\n",
      "          \"parent_id\": \"1518227583606788096\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1518227583606788096\"\n",
      "     },\n",
      "     \"1501302504843218945\": {\n",
      "          \"parent_user\": \"Bull100X\",\n",
      "          \"parent_id\": \"1491979366724554757\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1491979366724554757\"\n",
      "     },\n",
      "     \"1609994700773289984\": {\n",
      "          \"parent_user\": \"ApolloX_Finance\",\n",
      "          \"parent_id\": \"1609701001547714561\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1609701001547714561\"\n",
      "     },\n",
      "     \"1518323786441019392\": {\n",
      "          \"parent_user\": \"NEST_Protocol\",\n",
      "          \"parent_id\": \"1516424499473879048\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1516424499473879048\"\n",
      "     },\n",
      "     \"1499232470306336769\": {\n",
      "          \"parent_user\": \"TopGoal_NFT\",\n",
      "          \"parent_id\": \"1460332367650832387\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1460332367650832387\"\n",
      "     },\n",
      "     \"1509112118603300873\": {\n",
      "          \"parent_user\": \"TheCryptoLaunch\",\n",
      "          \"parent_id\": \"1502680563622330372\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1502680563622330372\"\n",
      "     },\n",
      "     \"1500814547942514693\": {\n",
      "          \"parent_user\": \"Paragenio\",\n",
      "          \"parent_id\": \"1497549644448301057\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1497549644448301057\"\n",
      "     },\n",
      "     \"1501266286747262978\": {\n",
      "          \"parent_user\": \"MeBoxBSC\",\n",
      "          \"parent_id\": \"1494612640047403008\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1494612640047403008\"\n",
      "     },\n",
      "     \"1509484452870856705\": {\n",
      "          \"parent_user\": \"SoccerFan_io\",\n",
      "          \"parent_id\": \"1505925822800007170\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1505925822800007170\"\n",
      "     },\n",
      "     \"1507361434778353670\": {\n",
      "          \"parent_user\": \"TheCryptoLaunch\",\n",
      "          \"parent_id\": \"1502680563622330372\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1502680563622330372\"\n",
      "     },\n",
      "     \"1511581940108840963\": {\n",
      "          \"parent_user\": \"AvaultOmni\",\n",
      "          \"parent_id\": \"1507311786324033550\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1507311786324033550\"\n",
      "     },\n",
      "     \"1509519778700009478\": {\n",
      "          \"parent_user\": \"AirdropBSC_Com\",\n",
      "          \"parent_id\": \"1509368738713255944\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1509368738713255944\"\n",
      "     },\n",
      "     \"1507037651014414337\": {\n",
      "          \"parent_user\": \"REVDReverse\",\n",
      "          \"parent_id\": \"1500402939059339267\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1500402939059339267\"\n",
      "     },\n",
      "     \"1507048253451350019\": {\n",
      "          \"parent_user\": \"REVDReverse\",\n",
      "          \"parent_id\": \"1500402939059339267\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1500402939059339267\"\n",
      "     },\n",
      "     \"1499589788764348421\": {\n",
      "          \"parent_user\": \"ton_blockchain\",\n",
      "          \"parent_id\": \"1496456914057715714\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1496456914057715714\"\n",
      "     },\n",
      "     \"1585879174190727168\": {\n",
      "          \"parent_user\": \"apsocoin\",\n",
      "          \"parent_id\": \"1583020880065888256\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1583020880065888256\"\n",
      "     },\n",
      "     \"1511996583847419907\": {\n",
      "          \"parent_user\": \"StarbankFinance\",\n",
      "          \"parent_id\": \"1509192744119730179\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1509192744119730179\"\n",
      "     },\n",
      "     \"1501286227559043073\": {\n",
      "          \"parent_user\": \"metarobox\",\n",
      "          \"parent_id\": \"1495556777399955456\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1495556777399955456\"\n",
      "     },\n",
      "     \"1540879951850680320\": {\n",
      "          \"parent_user\": \"DesmoLabs\",\n",
      "          \"parent_id\": \"1536291068840493060\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1536291068840493060\"\n",
      "     },\n",
      "     \"1514881890507190272\": {\n",
      "          \"parent_user\": \"PinkPeaFinance\",\n",
      "          \"parent_id\": \"1513392154437369856\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1513392154437369856\"\n",
      "     },\n",
      "     \"1540879496387055616\": {\n",
      "          \"parent_user\": \"DesmoLabs\",\n",
      "          \"parent_id\": \"1536291068840493060\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1536291068840493060\"\n",
      "     },\n",
      "     \"1507018149358571521\": {\n",
      "          \"parent_user\": \"TheCryptoLaunch\",\n",
      "          \"parent_id\": \"1506893672020905991\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1506893672020905991\"\n",
      "     },\n",
      "     \"1558742737356472327\": {\n",
      "          \"parent_user\": \"SeedifyFund\",\n",
      "          \"parent_id\": \"1558742338410782720\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1558742338410782720\"\n",
      "     },\n",
      "     \"1501231494060478464\": {\n",
      "          \"parent_user\": \"DriveCityNFT\",\n",
      "          \"parent_id\": \"1498573316314914818\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1498573316314914818\"\n",
      "     },\n",
      "     \"1515931281430917123\": {\n",
      "          \"parent_user\": \"jameszhao2023\",\n",
      "          \"parent_id\": \"1513831014040477706\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1513831014040477706\"\n",
      "     },\n",
      "     \"1599032274208493568\": {\n",
      "          \"parent_user\": \"staika_official\",\n",
      "          \"parent_id\": \"1596838717624238080\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1596838717624238080\"\n",
      "     },\n",
      "     \"1586263174604410880\": {\n",
      "          \"parent_user\": \"apsocoin\",\n",
      "          \"parent_id\": \"1583020880065888256\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1583020880065888256\"\n",
      "     },\n",
      "     \"1507023346193813506\": {\n",
      "          \"parent_user\": \"REVDReverse\",\n",
      "          \"parent_id\": \"1500402939059339267\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1500402939059339267\"\n",
      "     },\n",
      "     \"1507277206501818370\": {\n",
      "          \"parent_user\": \"REVDReverse\",\n",
      "          \"parent_id\": \"1500402939059339267\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1500402939059339267\"\n",
      "     },\n",
      "     \"1514778895442407425\": {\n",
      "          \"parent_user\": \"PlayCryptoBallZ\",\n",
      "          \"parent_id\": \"1508981405229129728\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1508981405229129728\"\n",
      "     },\n",
      "     \"1511527510864891908\": {\n",
      "          \"parent_user\": \"AvaultOmni\",\n",
      "          \"parent_id\": \"1507311786324033550\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1507311786324033550\"\n",
      "     },\n",
      "     \"1501406145595863041\": {\n",
      "          \"parent_user\": \"Bull100X\",\n",
      "          \"parent_id\": \"1491605958971191298\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1491605958971191298\"\n",
      "     },\n",
      "     \"1514918723798601728\": {\n",
      "          \"parent_user\": \"PinkPeaFinance\",\n",
      "          \"parent_id\": \"1513392154437369856\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1513392154437369856\"\n",
      "     },\n",
      "     \"1500735012345237505\": {\n",
      "          \"parent_user\": \"playermons\",\n",
      "          \"parent_id\": \"1496757407963181057\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1496757407963181057\"\n",
      "     },\n",
      "     \"1518981408425148416\": {\n",
      "          \"parent_user\": \"DOGB_Token\",\n",
      "          \"parent_id\": \"1515684055773294593\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1515684055773294593\"\n",
      "     },\n",
      "     \"1501195037631258626\": {\n",
      "          \"parent_user\": \"DriveCityNFT\",\n",
      "          \"parent_id\": \"1498573316314914818\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1498573316314914818\"\n",
      "     },\n",
      "     \"1509501089447657479\": {\n",
      "          \"parent_user\": \"AirdropDaily_\",\n",
      "          \"parent_id\": \"1502980257275875330\",\n",
      "          \"parent_link\": \"https://twitter.com/anyuser/status/1502980257275875330\"\n",
      "     }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "json_file = \"replies_to_100_most_retweeted_users_good.json\"\n",
    "f = open(json_file)\n",
    "file = json.load(f)\n",
    "f.close() \n",
    "dictionary1 = dict()\n",
    "names_list = list()\n",
    "for item in file:\n",
    "    names_list.append(item)\n",
    "i = 0\n",
    "while i <= 99:\n",
    "    \n",
    "    index = random.randint(0,len(names_list) - 1)\n",
    "    sub = names_list[index]\n",
    "    \n",
    "    if 'data' in file[sub]:\n",
    "        ind2 = random.randint(0,len(file[sub]['data']) - 1)\n",
    "        if 'referenced_tweets' in file[sub]['data'][ind2]:\n",
    "            #print(len(file[names_list[index]]['data']))\n",
    "            #print(ind2)\n",
    "            if only_special_words(file[sub]['data'][ind2]['text']) and (len(file[sub]['data'][ind2]['entities']['mentions']) - 1 >= 3):\n",
    "                if((file[sub]['data'][ind2]['id']) not in dictionary1):\n",
    "                    dictionary1[file[sub]['data'][ind2]['id']] = {'parent_user': sub, 'parent_id': file[sub]['data'][ind2]['referenced_tweets'][0]['id'], 'parent_link': f\"https://twitter.com/anyuser/status/{file[sub]['data'][ind2]['referenced_tweets'][0]['id']}\"}\n",
    "                    i += 1\n",
    "                    \n",
    "\n",
    "\n",
    "print(json.dumps(dictionary1,indent=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('top_105_retweeted_users.json')\n",
    "data_file = json.load(f)\n",
    "f.close()\n",
    "creates_json_file_of_replies_to_top_users(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 2022-02-01T00:00:00Z --- 2: 2022-02-02T00:00:00Z\n",
      "1: 2022-02-02T00:00:00Z --- 2: 2022-02-03T00:00:00Z\n",
      "1: 2022-02-03T00:00:00Z --- 2: 2022-02-04T00:00:00Z\n",
      "1: 2022-02-04T00:00:00Z --- 2: 2022-02-05T00:00:00Z\n",
      "1: 2022-02-05T00:00:00Z --- 2: 2022-02-06T00:00:00Z\n",
      "1: 2022-02-06T00:00:00Z --- 2: 2022-02-07T00:00:00Z\n",
      "1: 2022-02-07T00:00:00Z --- 2: 2022-02-08T00:00:00Z\n",
      "1: 2022-02-08T00:00:00Z --- 2: 2022-02-09T00:00:00Z\n",
      "1: 2022-02-09T00:00:00Z --- 2: 2022-02-10T00:00:00Z\n",
      "1: 2022-02-10T00:00:00Z --- 2: 2022-02-11T00:00:00Z\n",
      "1: 2022-02-11T00:00:00Z --- 2: 2022-02-12T00:00:00Z\n",
      "1: 2022-02-12T00:00:00Z --- 2: 2022-02-13T00:00:00Z\n",
      "1: 2022-02-13T00:00:00Z --- 2: 2022-02-14T00:00:00Z\n",
      "1: 2022-02-14T00:00:00Z --- 2: 2022-02-15T00:00:00Z\n",
      "1: 2022-02-15T00:00:00Z --- 2: 2022-02-16T00:00:00Z\n",
      "1: 2022-02-16T00:00:00Z --- 2: 2022-02-17T00:00:00Z\n",
      "1: 2022-02-17T00:00:00Z --- 2: 2022-02-18T00:00:00Z\n",
      "1: 2022-02-18T00:00:00Z --- 2: 2022-02-19T00:00:00Z\n",
      "1: 2022-02-19T00:00:00Z --- 2: 2022-02-20T00:00:00Z\n",
      "1: 2022-02-20T00:00:00Z --- 2: 2022-02-21T00:00:00Z\n",
      "1: 2022-02-21T00:00:00Z --- 2: 2022-02-22T00:00:00Z\n",
      "1: 2022-02-22T00:00:00Z --- 2: 2022-02-23T00:00:00Z\n",
      "1: 2022-02-23T00:00:00Z --- 2: 2022-02-24T00:00:00Z\n",
      "1: 2022-02-24T00:00:00Z --- 2: 2022-02-25T00:00:00Z\n",
      "1: 2022-02-25T00:00:00Z --- 2: 2022-02-26T00:00:00Z\n",
      "1: 2022-02-26T00:00:00Z --- 2: 2022-02-27T00:00:00Z\n",
      "1: 2022-02-27T00:00:00Z --- 2: 2022-02-28T00:00:00Z\n",
      "1: 2022-02-28T00:00:00Z --- 2: 2022-03-01T00:00:00Z\n",
      "1: 2022-03-01T00:00:00Z --- 2: 2022-03-02T00:00:00Z\n",
      "1: 2022-03-02T00:00:00Z --- 2: 2022-03-03T00:00:00Z\n",
      "1: 2022-03-03T00:00:00Z --- 2: 2022-03-04T00:00:00Z\n",
      "1: 2022-03-04T00:00:00Z --- 2: 2022-03-05T00:00:00Z\n",
      "1: 2022-03-05T00:00:00Z --- 2: 2022-03-06T00:00:00Z\n",
      "1: 2022-03-06T00:00:00Z --- 2: 2022-03-07T00:00:00Z\n",
      "1: 2022-03-07T00:00:00Z --- 2: 2022-03-08T00:00:00Z\n",
      "1: 2022-03-08T00:00:00Z --- 2: 2022-03-09T00:00:00Z\n",
      "1: 2022-03-09T00:00:00Z --- 2: 2022-03-10T00:00:00Z\n",
      "1: 2022-03-10T00:00:00Z --- 2: 2022-03-11T00:00:00Z\n",
      "1: 2022-03-11T00:00:00Z --- 2: 2022-03-12T00:00:00Z\n",
      "1: 2022-03-12T00:00:00Z --- 2: 2022-03-13T00:00:00Z\n",
      "1: 2022-03-13T00:00:00Z --- 2: 2022-03-14T00:00:00Z\n",
      "1: 2022-03-14T00:00:00Z --- 2: 2022-03-15T00:00:00Z\n",
      "1: 2022-03-15T00:00:00Z --- 2: 2022-03-16T00:00:00Z\n",
      "1: 2022-03-16T00:00:00Z --- 2: 2022-03-17T00:00:00Z\n",
      "1: 2022-03-17T00:00:00Z --- 2: 2022-03-18T00:00:00Z\n",
      "1: 2022-03-18T00:00:00Z --- 2: 2022-03-19T00:00:00Z\n",
      "1: 2022-03-19T00:00:00Z --- 2: 2022-03-20T00:00:00Z\n",
      "1: 2022-03-20T00:00:00Z --- 2: 2022-03-21T00:00:00Z\n",
      "1: 2022-03-21T00:00:00Z --- 2: 2022-03-22T00:00:00Z\n",
      "1: 2022-03-22T00:00:00Z --- 2: 2022-03-23T00:00:00Z\n",
      "1: 2022-03-23T00:00:00Z --- 2: 2022-03-24T00:00:00Z\n",
      "1: 2022-03-24T00:00:00Z --- 2: 2022-03-25T00:00:00Z\n",
      "1: 2022-03-25T00:00:00Z --- 2: 2022-03-26T00:00:00Z\n",
      "1: 2022-03-26T00:00:00Z --- 2: 2022-03-27T00:00:00Z\n",
      "1: 2022-03-27T00:00:00Z --- 2: 2022-03-28T00:00:00Z\n",
      "1: 2022-03-28T00:00:00Z --- 2: 2022-03-29T00:00:00Z\n",
      "1: 2022-03-29T00:00:00Z --- 2: 2022-03-30T00:00:00Z\n",
      "1: 2022-03-30T00:00:00Z --- 2: 2022-03-31T00:00:00Z\n",
      "1: 2022-03-31T00:00:00Z --- 2: 2022-04-01T00:00:00Z\n",
      "1: 2022-04-01T00:00:00Z --- 2: 2022-04-02T00:00:00Z\n",
      "1: 2022-04-02T00:00:00Z --- 2: 2022-04-03T00:00:00Z\n",
      "1: 2022-04-03T00:00:00Z --- 2: 2022-04-04T00:00:00Z\n",
      "1: 2022-04-04T00:00:00Z --- 2: 2022-04-05T00:00:00Z\n",
      "1: 2022-04-05T00:00:00Z --- 2: 2022-04-06T00:00:00Z\n",
      "1: 2022-04-06T00:00:00Z --- 2: 2022-04-07T00:00:00Z\n",
      "1: 2022-04-07T00:00:00Z --- 2: 2022-04-08T00:00:00Z\n",
      "1: 2022-04-08T00:00:00Z --- 2: 2022-04-09T00:00:00Z\n",
      "1: 2022-04-09T00:00:00Z --- 2: 2022-04-10T00:00:00Z\n",
      "1: 2022-04-10T00:00:00Z --- 2: 2022-04-11T00:00:00Z\n",
      "1: 2022-04-11T00:00:00Z --- 2: 2022-04-12T00:00:00Z\n",
      "1: 2022-04-12T00:00:00Z --- 2: 2022-04-13T00:00:00Z\n",
      "1: 2022-04-13T00:00:00Z --- 2: 2022-04-14T00:00:00Z\n",
      "1: 2022-04-14T00:00:00Z --- 2: 2022-04-15T00:00:00Z\n",
      "1: 2022-04-15T00:00:00Z --- 2: 2022-04-16T00:00:00Z\n",
      "1: 2022-04-16T00:00:00Z --- 2: 2022-04-17T00:00:00Z\n",
      "1: 2022-04-17T00:00:00Z --- 2: 2022-04-18T00:00:00Z\n",
      "1: 2022-04-18T00:00:00Z --- 2: 2022-04-19T00:00:00Z\n",
      "1: 2022-04-19T00:00:00Z --- 2: 2022-04-20T00:00:00Z\n",
      "1: 2022-04-20T00:00:00Z --- 2: 2022-04-21T00:00:00Z\n",
      "1: 2022-04-21T00:00:00Z --- 2: 2022-04-22T00:00:00Z\n",
      "1: 2022-04-22T00:00:00Z --- 2: 2022-04-23T00:00:00Z\n",
      "1: 2022-04-23T00:00:00Z --- 2: 2022-04-24T00:00:00Z\n",
      "1: 2022-04-24T00:00:00Z --- 2: 2022-04-25T00:00:00Z\n",
      "1: 2022-04-25T00:00:00Z --- 2: 2022-04-26T00:00:00Z\n",
      "1: 2022-04-26T00:00:00Z --- 2: 2022-04-27T00:00:00Z\n",
      "1: 2022-04-27T00:00:00Z --- 2: 2022-04-28T00:00:00Z\n",
      "1: 2022-04-28T00:00:00Z --- 2: 2022-04-29T00:00:00Z\n",
      "1: 2022-04-29T00:00:00Z --- 2: 2022-04-30T00:00:00Z\n",
      "1: 2022-04-30T00:00:00Z --- 2: 2022-05-01T00:00:00Z\n",
      "1: 2022-05-01T00:00:00Z --- 2: 2022-05-02T00:00:00Z\n",
      "1: 2022-05-02T00:00:00Z --- 2: 2022-05-03T00:00:00Z\n",
      "1: 2022-05-03T00:00:00Z --- 2: 2022-05-04T00:00:00Z\n",
      "1: 2022-05-04T00:00:00Z --- 2: 2022-05-05T00:00:00Z\n",
      "1: 2022-05-05T00:00:00Z --- 2: 2022-05-06T00:00:00Z\n",
      "1: 2022-05-06T00:00:00Z --- 2: 2022-05-07T00:00:00Z\n",
      "1: 2022-05-07T00:00:00Z --- 2: 2022-05-08T00:00:00Z\n",
      "1: 2022-05-08T00:00:00Z --- 2: 2022-05-09T00:00:00Z\n",
      "1: 2022-05-09T00:00:00Z --- 2: 2022-05-10T00:00:00Z\n",
      "1: 2022-05-10T00:00:00Z --- 2: 2022-05-11T00:00:00Z\n",
      "1: 2022-05-11T00:00:00Z --- 2: 2022-05-12T00:00:00Z\n",
      "1: 2022-05-12T00:00:00Z --- 2: 2022-05-13T00:00:00Z\n",
      "1: 2022-05-13T00:00:00Z --- 2: 2022-05-14T00:00:00Z\n",
      "1: 2022-05-14T00:00:00Z --- 2: 2022-05-15T00:00:00Z\n",
      "1: 2022-05-15T00:00:00Z --- 2: 2022-05-16T00:00:00Z\n",
      "1: 2022-05-16T00:00:00Z --- 2: 2022-05-17T00:00:00Z\n",
      "1: 2022-05-17T00:00:00Z --- 2: 2022-05-18T00:00:00Z\n",
      "1: 2022-05-18T00:00:00Z --- 2: 2022-05-19T00:00:00Z\n",
      "1: 2022-05-19T00:00:00Z --- 2: 2022-05-20T00:00:00Z\n",
      "1: 2022-05-20T00:00:00Z --- 2: 2022-05-21T00:00:00Z\n",
      "1: 2022-05-21T00:00:00Z --- 2: 2022-05-22T00:00:00Z\n",
      "1: 2022-05-22T00:00:00Z --- 2: 2022-05-23T00:00:00Z\n",
      "1: 2022-05-23T00:00:00Z --- 2: 2022-05-24T00:00:00Z\n",
      "1: 2022-05-24T00:00:00Z --- 2: 2022-05-25T00:00:00Z\n",
      "1: 2022-05-25T00:00:00Z --- 2: 2022-05-26T00:00:00Z\n",
      "1: 2022-05-26T00:00:00Z --- 2: 2022-05-27T00:00:00Z\n",
      "1: 2022-05-27T00:00:00Z --- 2: 2022-05-28T00:00:00Z\n",
      "1: 2022-05-28T00:00:00Z --- 2: 2022-05-29T00:00:00Z\n",
      "1: 2022-05-29T00:00:00Z --- 2: 2022-05-30T00:00:00Z\n",
      "1: 2022-05-30T00:00:00Z --- 2: 2022-05-31T00:00:00Z\n",
      "1: 2022-05-31T00:00:00Z --- 2: 2022-06-01T00:00:00Z\n",
      "1: 2022-06-01T00:00:00Z --- 2: 2022-06-02T00:00:00Z\n",
      "1: 2022-06-02T00:00:00Z --- 2: 2022-06-03T00:00:00Z\n",
      "1: 2022-06-03T00:00:00Z --- 2: 2022-06-04T00:00:00Z\n",
      "1: 2022-06-04T00:00:00Z --- 2: 2022-06-05T00:00:00Z\n",
      "1: 2022-06-05T00:00:00Z --- 2: 2022-06-06T00:00:00Z\n",
      "1: 2022-06-06T00:00:00Z --- 2: 2022-06-07T00:00:00Z\n",
      "1: 2022-06-07T00:00:00Z --- 2: 2022-06-08T00:00:00Z\n",
      "1: 2022-06-08T00:00:00Z --- 2: 2022-06-09T00:00:00Z\n",
      "1: 2022-06-09T00:00:00Z --- 2: 2022-06-10T00:00:00Z\n",
      "1: 2022-06-10T00:00:00Z --- 2: 2022-06-11T00:00:00Z\n",
      "1: 2022-06-11T00:00:00Z --- 2: 2022-06-12T00:00:00Z\n",
      "1: 2022-06-12T00:00:00Z --- 2: 2022-06-13T00:00:00Z\n",
      "1: 2022-06-13T00:00:00Z --- 2: 2022-06-14T00:00:00Z\n",
      "1: 2022-06-14T00:00:00Z --- 2: 2022-06-15T00:00:00Z\n",
      "1: 2022-06-15T00:00:00Z --- 2: 2022-06-16T00:00:00Z\n",
      "1: 2022-06-16T00:00:00Z --- 2: 2022-06-17T00:00:00Z\n",
      "1: 2022-06-17T00:00:00Z --- 2: 2022-06-18T00:00:00Z\n",
      "1: 2022-06-18T00:00:00Z --- 2: 2022-06-19T00:00:00Z\n",
      "1: 2022-06-19T00:00:00Z --- 2: 2022-06-20T00:00:00Z\n",
      "1: 2022-06-20T00:00:00Z --- 2: 2022-06-21T00:00:00Z\n",
      "1: 2022-06-21T00:00:00Z --- 2: 2022-06-22T00:00:00Z\n",
      "1: 2022-06-22T00:00:00Z --- 2: 2022-06-23T00:00:00Z\n",
      "1: 2022-06-23T00:00:00Z --- 2: 2022-06-24T00:00:00Z\n",
      "1: 2022-06-24T00:00:00Z --- 2: 2022-06-25T00:00:00Z\n",
      "1: 2022-06-25T00:00:00Z --- 2: 2022-06-26T00:00:00Z\n",
      "1: 2022-06-26T00:00:00Z --- 2: 2022-06-27T00:00:00Z\n",
      "1: 2022-06-27T00:00:00Z --- 2: 2022-06-28T00:00:00Z\n",
      "1: 2022-06-28T00:00:00Z --- 2: 2022-06-29T00:00:00Z\n",
      "1: 2022-06-29T00:00:00Z --- 2: 2022-06-30T00:00:00Z\n",
      "1: 2022-06-30T00:00:00Z --- 2: 2022-07-01T00:00:00Z\n",
      "1: 2022-07-01T00:00:00Z --- 2: 2022-07-02T00:00:00Z\n",
      "1: 2022-07-02T00:00:00Z --- 2: 2022-07-03T00:00:00Z\n",
      "1: 2022-07-03T00:00:00Z --- 2: 2022-07-04T00:00:00Z\n",
      "1: 2022-07-04T00:00:00Z --- 2: 2022-07-05T00:00:00Z\n",
      "1: 2022-07-05T00:00:00Z --- 2: 2022-07-06T00:00:00Z\n",
      "1: 2022-07-06T00:00:00Z --- 2: 2022-07-07T00:00:00Z\n",
      "1: 2022-07-07T00:00:00Z --- 2: 2022-07-08T00:00:00Z\n",
      "1: 2022-07-08T00:00:00Z --- 2: 2022-07-09T00:00:00Z\n",
      "1: 2022-07-09T00:00:00Z --- 2: 2022-07-10T00:00:00Z\n",
      "1: 2022-07-10T00:00:00Z --- 2: 2022-07-11T00:00:00Z\n",
      "1: 2022-07-11T00:00:00Z --- 2: 2022-07-12T00:00:00Z\n",
      "1: 2022-07-12T00:00:00Z --- 2: 2022-07-13T00:00:00Z\n",
      "1: 2022-07-13T00:00:00Z --- 2: 2022-07-14T00:00:00Z\n",
      "1: 2022-07-14T00:00:00Z --- 2: 2022-07-15T00:00:00Z\n",
      "1: 2022-07-15T00:00:00Z --- 2: 2022-07-16T00:00:00Z\n",
      "1: 2022-07-16T00:00:00Z --- 2: 2022-07-17T00:00:00Z\n",
      "1: 2022-07-17T00:00:00Z --- 2: 2022-07-18T00:00:00Z\n",
      "1: 2022-07-18T00:00:00Z --- 2: 2022-07-19T00:00:00Z\n",
      "1: 2022-07-19T00:00:00Z --- 2: 2022-07-20T00:00:00Z\n",
      "1: 2022-07-20T00:00:00Z --- 2: 2022-07-21T00:00:00Z\n",
      "1: 2022-07-21T00:00:00Z --- 2: 2022-07-22T00:00:00Z\n",
      "1: 2022-07-22T00:00:00Z --- 2: 2022-07-23T00:00:00Z\n",
      "1: 2022-07-23T00:00:00Z --- 2: 2022-07-24T00:00:00Z\n",
      "1: 2022-07-24T00:00:00Z --- 2: 2022-07-25T00:00:00Z\n",
      "1: 2022-07-25T00:00:00Z --- 2: 2022-07-26T00:00:00Z\n",
      "1: 2022-07-26T00:00:00Z --- 2: 2022-07-27T00:00:00Z\n",
      "1: 2022-07-27T00:00:00Z --- 2: 2022-07-28T00:00:00Z\n",
      "1: 2022-07-28T00:00:00Z --- 2: 2022-07-29T00:00:00Z\n",
      "1: 2022-07-29T00:00:00Z --- 2: 2022-07-30T00:00:00Z\n",
      "1: 2022-07-30T00:00:00Z --- 2: 2022-07-31T00:00:00Z\n",
      "1: 2022-07-31T00:00:00Z --- 2: 2022-08-01T00:00:00Z\n",
      "1: 2022-08-01T00:00:00Z --- 2: 2022-08-02T00:00:00Z\n",
      "1: 2022-08-02T00:00:00Z --- 2: 2022-08-03T00:00:00Z\n",
      "1: 2022-08-03T00:00:00Z --- 2: 2022-08-04T00:00:00Z\n",
      "1: 2022-08-04T00:00:00Z --- 2: 2022-08-05T00:00:00Z\n",
      "1: 2022-08-05T00:00:00Z --- 2: 2022-08-06T00:00:00Z\n",
      "1: 2022-08-06T00:00:00Z --- 2: 2022-08-07T00:00:00Z\n",
      "1: 2022-08-07T00:00:00Z --- 2: 2022-08-08T00:00:00Z\n",
      "1: 2022-08-08T00:00:00Z --- 2: 2022-08-09T00:00:00Z\n",
      "1: 2022-08-09T00:00:00Z --- 2: 2022-08-10T00:00:00Z\n",
      "1: 2022-08-10T00:00:00Z --- 2: 2022-08-11T00:00:00Z\n",
      "1: 2022-08-11T00:00:00Z --- 2: 2022-08-12T00:00:00Z\n",
      "1: 2022-08-12T00:00:00Z --- 2: 2022-08-13T00:00:00Z\n",
      "1: 2022-08-13T00:00:00Z --- 2: 2022-08-14T00:00:00Z\n",
      "1: 2022-08-14T00:00:00Z --- 2: 2022-08-15T00:00:00Z\n",
      "1: 2022-08-15T00:00:00Z --- 2: 2022-08-16T00:00:00Z\n",
      "1: 2022-08-16T00:00:00Z --- 2: 2022-08-17T00:00:00Z\n",
      "1: 2022-08-17T00:00:00Z --- 2: 2022-08-18T00:00:00Z\n",
      "1: 2022-08-18T00:00:00Z --- 2: 2022-08-19T00:00:00Z\n",
      "1: 2022-08-19T00:00:00Z --- 2: 2022-08-20T00:00:00Z\n",
      "1: 2022-08-20T00:00:00Z --- 2: 2022-08-21T00:00:00Z\n",
      "1: 2022-08-21T00:00:00Z --- 2: 2022-08-22T00:00:00Z\n",
      "1: 2022-08-22T00:00:00Z --- 2: 2022-08-23T00:00:00Z\n",
      "1: 2022-08-23T00:00:00Z --- 2: 2022-08-24T00:00:00Z\n",
      "1: 2022-08-24T00:00:00Z --- 2: 2022-08-25T00:00:00Z\n",
      "1: 2022-08-25T00:00:00Z --- 2: 2022-08-26T00:00:00Z\n",
      "1: 2022-08-26T00:00:00Z --- 2: 2022-08-27T00:00:00Z\n",
      "1: 2022-08-27T00:00:00Z --- 2: 2022-08-28T00:00:00Z\n",
      "1: 2022-08-28T00:00:00Z --- 2: 2022-08-29T00:00:00Z\n",
      "1: 2022-08-29T00:00:00Z --- 2: 2022-08-30T00:00:00Z\n",
      "1: 2022-08-30T00:00:00Z --- 2: 2022-08-31T00:00:00Z\n",
      "1: 2022-08-31T00:00:00Z --- 2: 2022-09-01T00:00:00Z\n",
      "1: 2022-09-01T00:00:00Z --- 2: 2022-09-02T00:00:00Z\n",
      "1: 2022-09-02T00:00:00Z --- 2: 2022-09-03T00:00:00Z\n",
      "1: 2022-09-03T00:00:00Z --- 2: 2022-09-04T00:00:00Z\n",
      "1: 2022-09-04T00:00:00Z --- 2: 2022-09-05T00:00:00Z\n",
      "1: 2022-09-05T00:00:00Z --- 2: 2022-09-06T00:00:00Z\n",
      "1: 2022-09-06T00:00:00Z --- 2: 2022-09-07T00:00:00Z\n",
      "1: 2022-09-07T00:00:00Z --- 2: 2022-09-08T00:00:00Z\n",
      "1: 2022-09-08T00:00:00Z --- 2: 2022-09-09T00:00:00Z\n",
      "1: 2022-09-09T00:00:00Z --- 2: 2022-09-10T00:00:00Z\n",
      "1: 2022-09-10T00:00:00Z --- 2: 2022-09-11T00:00:00Z\n",
      "1: 2022-09-11T00:00:00Z --- 2: 2022-09-12T00:00:00Z\n",
      "1: 2022-09-12T00:00:00Z --- 2: 2022-09-13T00:00:00Z\n",
      "1: 2022-09-13T00:00:00Z --- 2: 2022-09-14T00:00:00Z\n",
      "1: 2022-09-14T00:00:00Z --- 2: 2022-09-15T00:00:00Z\n",
      "1: 2022-09-15T00:00:00Z --- 2: 2022-09-16T00:00:00Z\n",
      "1: 2022-09-16T00:00:00Z --- 2: 2022-09-17T00:00:00Z\n",
      "1: 2022-09-17T00:00:00Z --- 2: 2022-09-18T00:00:00Z\n",
      "1: 2022-09-18T00:00:00Z --- 2: 2022-09-19T00:00:00Z\n",
      "1: 2022-09-19T00:00:00Z --- 2: 2022-09-20T00:00:00Z\n",
      "1: 2022-09-20T00:00:00Z --- 2: 2022-09-21T00:00:00Z\n",
      "1: 2022-09-21T00:00:00Z --- 2: 2022-09-22T00:00:00Z\n",
      "1: 2022-09-22T00:00:00Z --- 2: 2022-09-23T00:00:00Z\n",
      "1: 2022-09-23T00:00:00Z --- 2: 2022-09-24T00:00:00Z\n",
      "1: 2022-09-24T00:00:00Z --- 2: 2022-09-25T00:00:00Z\n",
      "1: 2022-09-25T00:00:00Z --- 2: 2022-09-26T00:00:00Z\n",
      "1: 2022-09-26T00:00:00Z --- 2: 2022-09-27T00:00:00Z\n",
      "1: 2022-09-27T00:00:00Z --- 2: 2022-09-28T00:00:00Z\n",
      "1: 2022-09-28T00:00:00Z --- 2: 2022-09-29T00:00:00Z\n",
      "1: 2022-09-29T00:00:00Z --- 2: 2022-09-30T00:00:00Z\n",
      "1: 2022-09-30T00:00:00Z --- 2: 2022-10-01T00:00:00Z\n",
      "1: 2022-10-01T00:00:00Z --- 2: 2022-10-02T00:00:00Z\n",
      "1: 2022-10-02T00:00:00Z --- 2: 2022-10-03T00:00:00Z\n",
      "1: 2022-10-03T00:00:00Z --- 2: 2022-10-04T00:00:00Z\n",
      "1: 2022-10-04T00:00:00Z --- 2: 2022-10-05T00:00:00Z\n",
      "1: 2022-10-05T00:00:00Z --- 2: 2022-10-06T00:00:00Z\n",
      "1: 2022-10-06T00:00:00Z --- 2: 2022-10-07T00:00:00Z\n",
      "1: 2022-10-07T00:00:00Z --- 2: 2022-10-08T00:00:00Z\n",
      "1: 2022-10-08T00:00:00Z --- 2: 2022-10-09T00:00:00Z\n",
      "1: 2022-10-09T00:00:00Z --- 2: 2022-10-10T00:00:00Z\n",
      "1: 2022-10-10T00:00:00Z --- 2: 2022-10-11T00:00:00Z\n",
      "1: 2022-10-11T00:00:00Z --- 2: 2022-10-12T00:00:00Z\n",
      "1: 2022-10-12T00:00:00Z --- 2: 2022-10-13T00:00:00Z\n",
      "1: 2022-10-13T00:00:00Z --- 2: 2022-10-14T00:00:00Z\n",
      "1: 2022-10-14T00:00:00Z --- 2: 2022-10-15T00:00:00Z\n",
      "1: 2022-10-15T00:00:00Z --- 2: 2022-10-16T00:00:00Z\n",
      "1: 2022-10-16T00:00:00Z --- 2: 2022-10-17T00:00:00Z\n",
      "1: 2022-10-17T00:00:00Z --- 2: 2022-10-18T00:00:00Z\n",
      "1: 2022-10-18T00:00:00Z --- 2: 2022-10-19T00:00:00Z\n",
      "1: 2022-10-19T00:00:00Z --- 2: 2022-10-20T00:00:00Z\n",
      "1: 2022-10-20T00:00:00Z --- 2: 2022-10-21T00:00:00Z\n",
      "1: 2022-10-21T00:00:00Z --- 2: 2022-10-22T00:00:00Z\n",
      "1: 2022-10-22T00:00:00Z --- 2: 2022-10-23T00:00:00Z\n",
      "1: 2022-10-23T00:00:00Z --- 2: 2022-10-24T00:00:00Z\n",
      "1: 2022-10-24T00:00:00Z --- 2: 2022-10-25T00:00:00Z\n",
      "1: 2022-10-25T00:00:00Z --- 2: 2022-10-26T00:00:00Z\n",
      "1: 2022-10-26T00:00:00Z --- 2: 2022-10-27T00:00:00Z\n",
      "1: 2022-10-27T00:00:00Z --- 2: 2022-10-28T00:00:00Z\n",
      "1: 2022-10-28T00:00:00Z --- 2: 2022-10-29T00:00:00Z\n",
      "1: 2022-10-29T00:00:00Z --- 2: 2022-10-30T00:00:00Z\n",
      "1: 2022-10-30T00:00:00Z --- 2: 2022-10-31T00:00:00Z\n",
      "1: 2022-10-31T00:00:00Z --- 2: 2022-11-01T00:00:00Z\n",
      "1: 2022-11-01T00:00:00Z --- 2: 2022-11-02T00:00:00Z\n",
      "1: 2022-11-02T00:00:00Z --- 2: 2022-11-03T00:00:00Z\n",
      "1: 2022-11-03T00:00:00Z --- 2: 2022-11-04T00:00:00Z\n",
      "1: 2022-11-04T00:00:00Z --- 2: 2022-11-05T00:00:00Z\n",
      "1: 2022-11-05T00:00:00Z --- 2: 2022-11-06T00:00:00Z\n",
      "1: 2022-11-06T00:00:00Z --- 2: 2022-11-07T00:00:00Z\n",
      "1: 2022-11-07T00:00:00Z --- 2: 2022-11-08T00:00:00Z\n",
      "1: 2022-11-08T00:00:00Z --- 2: 2022-11-09T00:00:00Z\n",
      "1: 2022-11-09T00:00:00Z --- 2: 2022-11-10T00:00:00Z\n",
      "1: 2022-11-10T00:00:00Z --- 2: 2022-11-11T00:00:00Z\n",
      "1: 2022-11-11T00:00:00Z --- 2: 2022-11-12T00:00:00Z\n",
      "1: 2022-11-12T00:00:00Z --- 2: 2022-11-13T00:00:00Z\n",
      "1: 2022-11-13T00:00:00Z --- 2: 2022-11-14T00:00:00Z\n",
      "1: 2022-11-14T00:00:00Z --- 2: 2022-11-15T00:00:00Z\n",
      "1: 2022-11-15T00:00:00Z --- 2: 2022-11-16T00:00:00Z\n",
      "1: 2022-11-16T00:00:00Z --- 2: 2022-11-17T00:00:00Z\n",
      "1: 2022-11-17T00:00:00Z --- 2: 2022-11-18T00:00:00Z\n",
      "1: 2022-11-18T00:00:00Z --- 2: 2022-11-19T00:00:00Z\n",
      "1: 2022-11-19T00:00:00Z --- 2: 2022-11-20T00:00:00Z\n",
      "1: 2022-11-20T00:00:00Z --- 2: 2022-11-21T00:00:00Z\n",
      "1: 2022-11-21T00:00:00Z --- 2: 2022-11-22T00:00:00Z\n",
      "1: 2022-11-22T00:00:00Z --- 2: 2022-11-23T00:00:00Z\n",
      "1: 2022-11-23T00:00:00Z --- 2: 2022-11-24T00:00:00Z\n",
      "1: 2022-11-24T00:00:00Z --- 2: 2022-11-25T00:00:00Z\n",
      "1: 2022-11-25T00:00:00Z --- 2: 2022-11-26T00:00:00Z\n",
      "1: 2022-11-26T00:00:00Z --- 2: 2022-11-27T00:00:00Z\n",
      "1: 2022-11-27T00:00:00Z --- 2: 2022-11-28T00:00:00Z\n",
      "1: 2022-11-28T00:00:00Z --- 2: 2022-11-29T00:00:00Z\n",
      "1: 2022-11-29T00:00:00Z --- 2: 2022-11-30T00:00:00Z\n",
      "1: 2022-11-30T00:00:00Z --- 2: 2022-12-01T00:00:00Z\n",
      "1: 2022-12-01T00:00:00Z --- 2: 2022-12-02T00:00:00Z\n",
      "1: 2022-12-02T00:00:00Z --- 2: 2022-12-03T00:00:00Z\n",
      "1: 2022-12-03T00:00:00Z --- 2: 2022-12-04T00:00:00Z\n",
      "1: 2022-12-04T00:00:00Z --- 2: 2022-12-05T00:00:00Z\n",
      "1: 2022-12-05T00:00:00Z --- 2: 2022-12-06T00:00:00Z\n",
      "1: 2022-12-06T00:00:00Z --- 2: 2022-12-07T00:00:00Z\n",
      "1: 2022-12-07T00:00:00Z --- 2: 2022-12-08T00:00:00Z\n",
      "1: 2022-12-08T00:00:00Z --- 2: 2022-12-09T00:00:00Z\n",
      "1: 2022-12-09T00:00:00Z --- 2: 2022-12-10T00:00:00Z\n",
      "1: 2022-12-10T00:00:00Z --- 2: 2022-12-11T00:00:00Z\n",
      "1: 2022-12-11T00:00:00Z --- 2: 2022-12-12T00:00:00Z\n",
      "1: 2022-12-12T00:00:00Z --- 2: 2022-12-13T00:00:00Z\n",
      "1: 2022-12-13T00:00:00Z --- 2: 2022-12-14T00:00:00Z\n",
      "1: 2022-12-14T00:00:00Z --- 2: 2022-12-15T00:00:00Z\n",
      "1: 2022-12-15T00:00:00Z --- 2: 2022-12-16T00:00:00Z\n",
      "1: 2022-12-16T00:00:00Z --- 2: 2022-12-17T00:00:00Z\n",
      "1: 2022-12-17T00:00:00Z --- 2: 2022-12-18T00:00:00Z\n",
      "1: 2022-12-18T00:00:00Z --- 2: 2022-12-19T00:00:00Z\n",
      "1: 2022-12-19T00:00:00Z --- 2: 2022-12-20T00:00:00Z\n",
      "1: 2022-12-20T00:00:00Z --- 2: 2022-12-21T00:00:00Z\n",
      "1: 2022-12-21T00:00:00Z --- 2: 2022-12-22T00:00:00Z\n",
      "1: 2022-12-22T00:00:00Z --- 2: 2022-12-23T00:00:00Z\n",
      "1: 2022-12-23T00:00:00Z --- 2: 2022-12-24T00:00:00Z\n",
      "1: 2022-12-24T00:00:00Z --- 2: 2022-12-25T00:00:00Z\n",
      "1: 2022-12-25T00:00:00Z --- 2: 2022-12-26T00:00:00Z\n",
      "1: 2022-12-26T00:00:00Z --- 2: 2022-12-27T00:00:00Z\n",
      "1: 2022-12-27T00:00:00Z --- 2: 2022-12-28T00:00:00Z\n",
      "1: 2022-12-28T00:00:00Z --- 2: 2022-12-29T00:00:00Z\n",
      "1: 2022-12-29T00:00:00Z --- 2: 2022-12-30T00:00:00Z\n",
      "1: 2022-12-30T00:00:00Z --- 2: 2022-12-31T00:00:00Z\n",
      "1: 2022-12-31T00:00:00Z --- 2: 2023-01-01T00:00:00Z\n",
      "1: 2023-01-01T00:00:00Z --- 2: 2023-01-02T00:00:00Z\n",
      "1: 2023-01-02T00:00:00Z --- 2: 2023-01-03T00:00:00Z\n",
      "1: 2023-01-03T00:00:00Z --- 2: 2023-01-04T00:00:00Z\n",
      "1: 2023-01-04T00:00:00Z --- 2: 2023-01-05T00:00:00Z\n",
      "1: 2023-01-05T00:00:00Z --- 2: 2023-01-06T00:00:00Z\n",
      "1: 2023-01-06T00:00:00Z --- 2: 2023-01-07T00:00:00Z\n",
      "1: 2023-01-07T00:00:00Z --- 2: 2023-01-08T00:00:00Z\n",
      "1: 2023-01-08T00:00:00Z --- 2: 2023-01-09T00:00:00Z\n",
      "1: 2023-01-09T00:00:00Z --- 2: 2023-01-10T00:00:00Z\n",
      "1: 2023-01-10T00:00:00Z --- 2: 2023-01-11T00:00:00Z\n",
      "1: 2023-01-11T00:00:00Z --- 2: 2023-01-12T00:00:00Z\n",
      "1: 2023-01-12T00:00:00Z --- 2: 2023-01-13T00:00:00Z\n",
      "1: 2023-01-13T00:00:00Z --- 2: 2023-01-14T00:00:00Z\n",
      "1: 2023-01-14T00:00:00Z --- 2: 2023-01-15T00:00:00Z\n",
      "1: 2023-01-15T00:00:00Z --- 2: 2023-01-16T00:00:00Z\n",
      "1: 2023-01-16T00:00:00Z --- 2: 2023-01-17T00:00:00Z\n",
      "1: 2023-01-17T00:00:00Z --- 2: 2023-01-18T00:00:00Z\n",
      "1: 2023-01-18T00:00:00Z --- 2: 2023-01-19T00:00:00Z\n",
      "1: 2023-01-19T00:00:00Z --- 2: 2023-01-20T00:00:00Z\n",
      "1: 2023-01-20T00:00:00Z --- 2: 2023-01-21T00:00:00Z\n",
      "1: 2023-01-21T00:00:00Z --- 2: 2023-01-22T00:00:00Z\n",
      "1: 2023-01-22T00:00:00Z --- 2: 2023-01-23T00:00:00Z\n",
      "1: 2023-01-23T00:00:00Z --- 2: 2023-01-24T00:00:00Z\n",
      "1: 2023-01-24T00:00:00Z --- 2: 2023-01-25T00:00:00Z\n",
      "1: 2023-01-25T00:00:00Z --- 2: 2023-01-26T00:00:00Z\n",
      "1: 2023-01-26T00:00:00Z --- 2: 2023-01-27T00:00:00Z\n",
      "1: 2023-01-27T00:00:00Z --- 2: 2023-01-28T00:00:00Z\n",
      "1: 2023-01-28T00:00:00Z --- 2: 2023-01-29T00:00:00Z\n",
      "1: 2023-01-29T00:00:00Z --- 2: 2023-01-30T00:00:00Z\n",
      "1: 2023-01-30T00:00:00Z --- 2: 2023-01-31T00:00:00Z\n",
      "1: 2023-01-31T00:00:00Z --- 2: 2023-02-01T00:00:00Z\n"
     ]
    }
   ],
   "source": [
    "oneyey, twowo = list_of_one_day_ranges_for_rfc_3339_since_specific_date()\n",
    "for i in range(365):\n",
    "    print(f\"1: {oneyey[i]} --- 2: {twowo[i]}\")\n",
    "#tweets_per_range()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
