{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First protocol\n",
    "_Code written and runs in python 3.11.0. Modify environment variables and queries as needed._  \n",
    "_Please use venv_\n",
    "\n",
    "## Protocol\n",
    "· Start with keywords:\n",
    "\n",
    "- Smartchain\n",
    "\n",
    "- Nft\n",
    "\n",
    "- Airdrop\n",
    "\n",
    "- Crypto\n",
    "\n",
    "- …etc.\n",
    "\n",
    "1. Sample up to 10k tweets containing at least one term from 100 random hours from the past year (so 1M tweets)\n",
    "\n",
    "2. Determine the most engaged (top) with users from this combined sample (100 or 1000)\n",
    "\n",
    "3. Pull up to 1000 comments for each top user\n",
    "\n",
    "4. Determine top users whose comments mention at least three users other than the top user\n",
    "\n",
    "5. Expand top user sample if we don’t have at least 100 airdrop seeders\n",
    "\n",
    "6. Time series chart plots:\n",
    "\n",
    "7. Top user activity\n",
    "\n",
    "8. Airdrop seeder activity\n",
    "\n",
    "9. Negative reaction activity? (based on sentiment analysis of replies to airdrop messages)\n",
    "\n",
    "10. External crypto value signals (from where?)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "Run the following commands in the terminal to install the required packages\n",
    "\n",
    "$pip install requests  \n",
    "  \n",
    "$pip install pandas  \n",
    "  \n",
    "$pip install datetime  \n",
    "  \n",
    "$pip install python-dateutil\n",
    "  \n",
    "\n",
    "--------------------  \n",
    "create a files <data.json> in the same folder as this jupyter notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Authentication step\n",
    "In the code cell below replace bearer_token with your bearer token. Run the cell, then delete your bearer token.\n",
    "This creates the token as an environment variable to be used under the name TOKEN. The token can then be removed so that others do not have access to your token when code is shared via GitHub. I will change this to dotenv and a .gitignore file later I just havent done that yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKEN'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This cell creates functions to be used for authentication as well as creating endpoints.\"\"\"\n",
    "import requests\n",
    "\n",
    "def auth():\n",
    "    \"\"\"Retrieves your bearer token.\"\"\"\n",
    "    return os.getenv('TOKEN')\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def create_full_search_url(keyword: str, start_date: list[str], end_date: list[str], max_results: int = 100):\n",
    "    \n",
    "    search_url: str = \"https://api.twitter.com/2/tweets/search/all\" \n",
    "\n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {'query': keyword,\n",
    "                    'start_time': start_date,\n",
    "                    'end_time': end_date,\n",
    "                    'max_results': max_results,\n",
    "                    'expansions': 'referenced_tweets.id.author_id',\n",
    "                    'tweet.fields': 'id,author_id,conversation_id,created_at,in_reply_to_user_id,lang,public_metrics,referenced_tweets,source,text',\n",
    "                    #'user.fields': 'id,name,public_metrics,username,verified',\n",
    "                    #'place.fields': 'country',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)\n",
    "\n",
    "def search_retweet_id_url(id: str):\n",
    "    \n",
    "    search_url: str = f\"https://api.twitter.com/2/tweets/{id}\" \n",
    "\n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {'tweet.fields': 'author_id'}\n",
    "    return (search_url, query_params)\n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"\\nEndpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "#print(auth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "\"\"\"This code cell contains two functions (is_leap_year and random_date) which help generate a random one hour date range when random_date() is called\"\"\"\n",
    "# Use by calling \"start_time, end_time = random_date()\"\n",
    "\n",
    "def is_leap_year(year):\n",
    "    \"\"\"Returns True if the given year is a leap year, False otherwise.\"\"\"\n",
    "    if year % 4 == 0:\n",
    "        if year % 100 == 0:\n",
    "            if year % 400 == 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def sort_timestamps(timestamps):\n",
    "    # Convert timestamps to datetime objects\n",
    "    datetimes = [datetime.datetime.fromisoformat(ts) for ts in timestamps]\n",
    "    # Sort datetime objects\n",
    "    datetimes.sort()\n",
    "    # Convert sorted datetime objects back to timestamps\n",
    "    sorted_timestamps = [dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\") for dt in datetimes]\n",
    "    return sorted_timestamps\n",
    "\n",
    "def random_date():\n",
    "    \"\"\"Generate a random one hour date range within the last year in RFC 3339 format to be used with twitter API.\"\"\"\n",
    "    month = random.randint(1, 12)\n",
    "    year = random.randint(datetime.datetime.now().year - 1, datetime.datetime.now().year)\n",
    "    if month <= datetime.datetime.now().month:\n",
    "        year = datetime.datetime.now().year\n",
    "    else:\n",
    "        year = datetime.datetime.now().year - 1\n",
    "    if month == datetime.datetime.now().month:\n",
    "        day = random.randint(1, datetime.datetime.now().day - 1)\n",
    "    elif month == 2:\n",
    "        if is_leap_year(year):\n",
    "            day = random.randint(1, 29)\n",
    "        else:\n",
    "            day = random.randint(1, 28)\n",
    "    elif month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "        day = random.randint(1, 31)\n",
    "    else:\n",
    "        day = random.randint(1, 30)\n",
    "    hour = random.randint(0, 23)\n",
    "    start_time = datetime.datetime(year, month, day, hour)\n",
    "    end_time = start_time + datetime.timedelta(hours=1)\n",
    "    start_timestamp = start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    end_timestamp = end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    return start_timestamp, end_timestamp\n",
    "\n",
    "def return_n_random_hour_ranges_sorted(n: int) -> list:\n",
    "    start_time1_list = list()\n",
    "    end_time1_list = list()\n",
    "    for i in range(0, n):\n",
    "        s1, s2 = random_date()\n",
    "        while s1 in start_time1_list:\n",
    "            s1, s2 = random_date()\n",
    "        start_time1_list.append(s1)\n",
    "        end_time1_list.append(s2)\n",
    "\n",
    "    sorted_start = sort_timestamps(start_time1_list)\n",
    "    sorted_end = sort_timestamps(end_time1_list)\n",
    "    return (sorted_start, sorted_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"WORKING EXAMPLE OF SO FAR, Above is for more function use etc.\"\"\"\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "from typing import Optional\n",
    "import time\n",
    "\n",
    "def tweets_per_range(keyword: str, start_times_list: list, end_times_list: list, results_per_range: int, next_token: Optional[str] = None) -> None:\n",
    "    bearer_token = auth()\n",
    "    headers: dict[str, str] = create_headers(bearer_token)\n",
    "    json_obj_by_time_range: dict[str, dict] = dict()\n",
    "    max_results: int = 500\n",
    "    for i in range(0, len(start_times_list)):\n",
    "        total_count = 0\n",
    "        # Creates url and connects to endpoint then assignts the JSON STRING API response to json_response\n",
    "        url = create_full_search_url(keyword, start_times_list[i], end_times_list[i], max_results)\n",
    "        json_obj_response = connect_to_endpoint(url[0], headers, url[1], next_token) # prints response code\n",
    "        print(f\"Outer for loop enpoint called for list index {i} / {len(start_times_list) - 1}\")\n",
    "        json_obj_response.pop('includes', None)\n",
    "        json_obj_response['time'] = (f\"{start_times_list[i]} --- {end_times_list[i]}\")\n",
    "\n",
    "        # Appends the json object API response to the json_obj_data dictionary.\n",
    "        json_obj_by_time_range[f'time_range_{i}'] = json_obj_response\n",
    "        total_count += json_obj_response['meta']['result_count']\n",
    "        time.sleep(5)\n",
    "        \n",
    "        while total_count <= results_per_range:\n",
    "            if 'next_token' in json_obj_response['meta']:\n",
    "                next_token = json_obj_response['meta']['next_token']\n",
    "\n",
    "                # Creates url and connects to endpoint then assignts the JSON API response to json_response\n",
    "                #url = create_full_search_url(keyword, start_times_list[i], end_times_list[i], max_results)\n",
    "                json_obj_response = connect_to_endpoint(url[0], headers, url[1], next_token) # prints response code\n",
    "                print(f\"While loop enpoint called: index {i} / {len(start_times_list) - 1}\")\n",
    "                next_token = None\n",
    "                if 'data' in json_obj_response:\n",
    "                    #if type(json_obj_response['data']) == list:\n",
    "                    for item in json_obj_response['data']:\n",
    "                        json_obj_by_time_range[f'time_range_{i}']['data'].append(item)\n",
    "                    #else:\n",
    "                        #json_obj_by_time_range[f'time_range_{i}']['data'].append(json_obj_response['data'])\n",
    "                    total_count += json_obj_response['meta']['result_count']\n",
    "                    json_obj_by_time_range[f'time_range_{i}']['meta']['result_count'] = total_count\n",
    "                    print(f\"data key found and data appended: {total_count} / {results_per_range} tweets in this range scraped\")\n",
    "                else:\n",
    "                    print(\"empty next token\")\n",
    "            else:\n",
    "                print(\"No more tweets to scrape, total tweets will be less than amount desired.\")\n",
    "                print(f\"total results {total_count}\")\n",
    "                next_token = None\n",
    "                break\n",
    "            time.sleep(5)\n",
    "        time.sleep(5)\n",
    "        json_to_file = json.dumps(json_obj_by_time_range)\n",
    "        with open(f\"data_range_{i}.json\", \"w\") as outfile:\n",
    "            outfile.write(json_to_file)\n",
    "        \n",
    "    #return(json_obj_by_time_range)\n",
    "        \n",
    "\n",
    "\"\"\"This function takes all of the json files of tweet data and creates a sorted dictionary of the most appearing tweets.\"\"\"\n",
    "\"\"\"A post OR a retweet counts as ONE occurence of a tweet.\"\"\"\n",
    "def analyze_top_appearing_tweets_in_data(max_int_of_json: int) -> dict:\n",
    "    tweet_metrics_dict: dict = dict()\n",
    "    for i in range(0, max_int_of_json + 1):\n",
    "        f = open(f'data_range_{i}.json')\n",
    "        data_file = json.load(f)\n",
    "        for item in data_file[f'time_range_{i}']['data']:\n",
    "            if 'referenced_tweets' in item and item['referenced_tweets'][0]['type'] == \"retweeted\":\n",
    "                original_tweet_id_from_retweet = item['referenced_tweets'][0]['id']\n",
    "                if original_tweet_id_from_retweet in tweet_metrics_dict:\n",
    "                    tweet_metrics_dict[original_tweet_id_from_retweet] += 1\n",
    "                else:\n",
    "                    tweet_metrics_dict[original_tweet_id_from_retweet] = 1\n",
    "            else:\n",
    "                this_tweet_id = item['id']\n",
    "                if this_tweet_id in tweet_metrics_dict:\n",
    "                    tweet_metrics_dict[this_tweet_id] += 1\n",
    "                else:\n",
    "                    tweet_metrics_dict[this_tweet_id] = 1\n",
    "        f.close()\n",
    "    #sorted(tweet_metrics_dict, key=tweet_metrics_dict.get, reverse=True)\n",
    "    #return(sorted(tweet_metrics_dict, key=tweet_metrics_dict.get, reverse=True))\n",
    "    sorted_dict = {}\n",
    "    sorted_keys = sorted(tweet_metrics_dict, key=tweet_metrics_dict.get, reverse=True)  #\n",
    "\n",
    "    for w in sorted_keys:\n",
    "        sorted_dict[w] = tweet_metrics_dict[w]\n",
    "    return sorted_dict\n",
    "\n",
    "\n",
    "def analyze_top_retweeted_tweets_in_data(max_int_of_json: int) -> dict:\n",
    "    tweet_metrics_dict: dict = dict()\n",
    "    for i in range(0, max_int_of_json + 1):\n",
    "        f = open(f'data_range_{i}.json')\n",
    "        data_file = json.load(f)\n",
    "        for item in data_file[f'time_range_{i}']['data']:\n",
    "            if 'referenced_tweets' in item and item['referenced_tweets'][0]['type'] == \"retweeted\":\n",
    "                retweet_count = item['public_metrics']['retweet_count']\n",
    "                original_tweet_id_from_retweet = item['referenced_tweets'][0]['id']\n",
    "                if original_tweet_id_from_retweet not in tweet_metrics_dict:\n",
    "                    tweet_metrics_dict[original_tweet_id_from_retweet] = retweet_count\n",
    "            else:\n",
    "                this_tweet_id = item['id']\n",
    "                if this_tweet_id not in tweet_metrics_dict:\n",
    "                    tweet_metrics_dict[this_tweet_id] = item['public_metrics']['retweet_count']\n",
    "        f.close()\n",
    "    #sorted(tweet_metrics_dict, key=tweet_metrics_dict.get, reverse=True)\n",
    "    #return(sorted(tweet_metrics_dict, key=tweet_metrics_dict.get, reverse=True))\n",
    "    sorted_dict = {}\n",
    "    sorted_keys = sorted(tweet_metrics_dict, key=tweet_metrics_dict.get, reverse=True)  #\n",
    "\n",
    "    for w in sorted_keys:\n",
    "        sorted_dict[w] = tweet_metrics_dict[w]\n",
    "    return sorted_dict\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Working Cell\n",
    "\n",
    "The below cell is intended to be the final cell which ties together all functions into the 'solution' to the protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_list, end_list = return_n_random_hour_ranges_sorted(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_list = [\"2023-01-10T17:00:00Z\", \"2023-01-11T17:00:00Z\"] #\"2023-01-08T17:00:00Z\", \n",
    "#end_list = [\"2023-01-10T18:00:00Z\", \"2023-01-11T18:00:00Z\"] #\"2023-01-08T18:00:00Z\",\n",
    "\n",
    "json_final_data = tweets_per_range(\"Smartchain OR Airdrop OR Crypto OR Nft\", start_list, end_list, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_test = analyze_top_retweeted_tweets_in_data(0)\n",
    "first_val = list(dict_test.values())[2]\n",
    "print(dict_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
