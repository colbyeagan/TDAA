{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First protocol\n",
    "_Code written and runs in python 3.11.0. Modify environment variables and queries as needed._  \n",
    "_Please use venv_\n",
    "\n",
    "## Protocol\n",
    "· Start with keywords:\n",
    "\n",
    "- Smartchain\n",
    "\n",
    "- Nft\n",
    "\n",
    "- Airdrop\n",
    "\n",
    "- Crypto\n",
    "\n",
    "- …etc.\n",
    "\n",
    "1. Sample up to 10k tweets containing at least one term from 100 random hours from the past year (so 1M tweets)\n",
    "\n",
    "2. Determine the most engaged (top) with users from this combined sample (100 or 1000)\n",
    "\n",
    "3. Pull up to 1000 comments for each top user\n",
    "\n",
    "4. Determine top users whose comments mention at least three users other than the top user\n",
    "\n",
    "5. Expand top user sample if we don’t have at least 100 airdrop seeders\n",
    "\n",
    "6. Time series chart plots:\n",
    "\n",
    "7. Top user activity\n",
    "\n",
    "8. Airdrop seeder activity\n",
    "\n",
    "9. Negative reaction activity? (based on sentiment analysis of replies to airdrop messages)\n",
    "\n",
    "10. External crypto value signals (from where?)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "Run the following commands in the terminal to install the required packages\n",
    "\n",
    "$pip install requests  \n",
    "  \n",
    "$pip install pandas  \n",
    "  \n",
    "$pip install datetime  \n",
    "  \n",
    "$pip install python-dateutil\n",
    "  \n",
    "\n",
    "--------------------  \n",
    "\n",
    "Crypto packages/api's:\n",
    "https://medium.com/codex/10-best-resources-to-fetch-cryptocurrency-data-in-python-8400cf0d0136\n",
    "\n",
    "https://www.alphavantage.co/documentation/\n",
    "\n",
    "\n",
    "Financial tools/packages:\n",
    "https://twitter.com/pyquantnews/status/1568029967052640256?t=EthvrNWmYhAFDVOhRoDxrQ&s=03\n",
    "\n",
    "https://pmorissette.github.io/ffn/quick.html#data-retrieval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Authentication step\n",
    "In the code cell below replace bearer_token with your bearer token. Run the cell, then delete your bearer token.\n",
    "This creates the token as an environment variable to be used under the name TOKEN. The token can then be removed so that others do not have access to your token when code is shared via GitHub. I will change this to dotenv and a .gitignore file later I just havent done that yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKEN'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "from typing import Optional\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import requests\n",
    "\n",
    "\"\"\"This code creates functions to be used for authentication as well as creating endpoints.\"\"\"\n",
    "def auth():\n",
    "    \"\"\"Retrieves your bearer token.\"\"\"\n",
    "    return os.getenv('TOKEN')\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    \"\"\"Creates headers for proper authentication from an API request.\"\"\"\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def create_full_search_url(keyword: str, start_date: str, end_date: str, max_results: int = 100):\n",
    "    \"\"\"Creates queries and params for a full archive search url.\"\"\"\n",
    "    search_url: str = \"https://api.twitter.com/2/tweets/search/all\" \n",
    "\n",
    "    # change params to desired params\n",
    "    query_params: dict = {'query': keyword,\n",
    "                    'start_time': start_date,\n",
    "                    'end_time': end_date,\n",
    "                    'max_results': max_results,\n",
    "                    'expansions': 'referenced_tweets.id.author_id',\n",
    "                    'tweet.fields': 'id,author_id,conversation_id,created_at,in_reply_to_user_id,lang,public_metrics,referenced_tweets,source,text',\n",
    "                    #'user.fields': 'id,name,public_metrics,username,verified',\n",
    "                    #'place.fields': 'country',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)\n",
    "\n",
    "def get_author_id_from_tweet_id(tweet_id: str):\n",
    "    \"\"\"Creates queries to find the author id of a tweet's author.\"\"\"\n",
    "    search_url: str = f\"https://api.twitter.com/2/tweets/{tweet_id}\" \n",
    "\n",
    "    # change params to desired params\n",
    "    query_params: dict = {'tweet.fields': 'author_id'}\n",
    "    b_tok = auth()\n",
    "    headers = create_headers(b_tok)\n",
    "    response = requests.request(\"GET\", search_url, headers = headers, params = query_params)\n",
    "    print(response.status_code)\n",
    "    json_response = response.json()\n",
    "    if 'data' in json_response:\n",
    "        return(response.json()['data']['author_id'])\n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    \"\"\"This takes a url from a url creation function and the params from the same function as well as an optional next token and returns a json object response from the endpoint.\"\"\"\n",
    "    params['next_token'] = next_token   # if a next token is found this will assign it to params 'next_token' key\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"\\nEndpoint Response Code: \" + str(response.status_code)) # prints the enpoint response code for debugging help\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "\"\"\"This code cell contains two functions (is_leap_year and random_date) which help generate a random one hour date range when random_date() is called\"\"\"\n",
    "def is_leap_year(year: int):\n",
    "    \"\"\"Returns True if the given year in typical four digit year format (i.e. 2023) is a leap year, False otherwise.\"\"\"\n",
    "    if year % 4 == 0:\n",
    "        if year % 100 == 0:\n",
    "            if year % 400 == 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def sort_timestamps(timestamps: list):\n",
    "    \"\"\"This function takes the list of start and end times in rfc 3339 format, returned from the return_n_random_hour_ranges_sorted function, and sorts the lists in chronological order.\"\"\"\n",
    "    # Convert timestamps to datetime objects\n",
    "    datetimes = [datetime.datetime.fromisoformat(ts) for ts in timestamps]\n",
    "    # Sort datetime objects\n",
    "    datetimes.sort()\n",
    "    # Convert sorted datetime objects back to timestamps\n",
    "    sorted_timestamps = [dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\") for dt in datetimes]\n",
    "    return sorted_timestamps\n",
    "\n",
    "def random_date():\n",
    "    \"\"\"Generate a random one hour date range within the last year in RFC 3339 format to be used with twitter API.\"\"\"\n",
    "    month = random.randint(1, 12)\n",
    "    year = random.randint(datetime.datetime.now().year - 1, datetime.datetime.now().year)\n",
    "    if month <= datetime.datetime.now().month:\n",
    "        year = datetime.datetime.now().year\n",
    "    else:\n",
    "        year = datetime.datetime.now().year - 1\n",
    "    if month == datetime.datetime.now().month:\n",
    "        if datetime.datetime.now().day <= 2:\n",
    "            day = 1\n",
    "        else:\n",
    "            day = random.randint(1, datetime.datetime.now().day - 1)\n",
    "    elif month == 2:\n",
    "        if is_leap_year(year):\n",
    "            day = random.randint(1, 29)\n",
    "        else:\n",
    "            day = random.randint(1, 28)\n",
    "    elif month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "        day = random.randint(1, 31)\n",
    "    else:\n",
    "        day = random.randint(1, 30)\n",
    "    hour = random.randint(0, 23)\n",
    "    start_time = datetime.datetime(year, month, day, hour)\n",
    "    end_time = start_time + datetime.timedelta(hours=1)\n",
    "    start_timestamp = start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    end_timestamp = end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    return start_timestamp, end_timestamp\n",
    "\n",
    "def return_n_random_hour_ranges_sorted(n: int) -> list:\n",
    "    \"\"\"This returns two SORTED lists of start times and end times (where each index of the end time is one hour after the same index in start time).\"\"\"\n",
    "    \"\"\"This function returns n number of one hour ranges.\"\"\"\n",
    "    start_time1_list: list = list()\n",
    "    end_time1_list: list = list()\n",
    "    for i in range(0, n):\n",
    "        s1, s2 = random_date()\n",
    "        while s1 in start_time1_list:\n",
    "            s1, s2 = random_date()\n",
    "        start_time1_list.append(s1)\n",
    "        end_time1_list.append(s2)\n",
    "\n",
    "    sorted_start = sort_timestamps(start_time1_list)\n",
    "    sorted_end = sort_timestamps(end_time1_list)\n",
    "    return (sorted_start, sorted_end)\n",
    "\n",
    "\n",
    "def tweets_per_range(keyword: str, start_times_list: list, end_times_list: list, results_per_range: int, next_token: Optional[str] = None) -> None:\n",
    "    \"\"\"This function takes a keyword(s), a list of start times and end times, and a integer amount of results per range.\"\"\"\n",
    "    \"\"\"It then creates json files for each time range and stores the tweet results in those json files.\"\"\"\n",
    "    \n",
    "    # AUTHENTICATION\n",
    "    bearer_token = auth()\n",
    "    headers: dict[str, str] = create_headers(bearer_token)\n",
    "    # AUTHENTICATION\n",
    "\n",
    "    json_obj_by_time_range: dict[str, dict] = dict() # creates a dictionary to which the time range will be created as a key and can therefore be found while in the json file\n",
    "    max_results: int = 500 # this is the max results per request the twitter API allows and should be left at 500\n",
    "    total_tweets_for_func_call: int = 0\n",
    "\n",
    "    # Loops through the time ranges in a list\n",
    "    for i in range(0, len(start_times_list)):\n",
    "        json_obj_by_time_range: dict[str, dict] = dict()\n",
    "        print(\"top of FOR loop\")\n",
    "        total_count = 0 # Tracks\n",
    "        \n",
    "        # Creates url and connects to endpoint then assignts the json object API response to json_obj_response\n",
    "        url = create_full_search_url(keyword, start_times_list[i], end_times_list[i], max_results)\n",
    "        json_obj_response = connect_to_endpoint(url[0], headers, url[1], next_token) # prints response code\n",
    "        print(f\"Outer for loop enpoint called for list index {i} / {len(start_times_list) - 1}\") # for quality control\n",
    "        json_obj_response.pop('includes', None) # removes 'includes' key which is a negative externality of calling 'referenced_tweets.id.author_id' expansion\n",
    "        json_obj_response['time'] = [start_times_list[i], end_times_list[i]] # adds a 'time' key to the json_obj_response so that the time range of all tweets can be found by calling json_object_response['time']\n",
    "\n",
    "        # Appends the json object API response to the json_obj_by_time_range dictionary\n",
    "        json_obj_by_time_range[f'time_range_{i}'] = json_obj_response\n",
    "        total_count += json_obj_response['meta']['result_count'] # increases the total count counter by the results count in the first API 'GET'\n",
    "        print(f\"endpoint called and data collected: {total_count} / {results_per_range} tweets in this range scraped\")\n",
    "        time.sleep(5) # time.sleep implementations are seen throughout the code to avoid hitting rate limits of twitter API\n",
    "        \n",
    "        # If there were insifficient results from first 'GET' request to meet the results per range value then the API begins to paginate to scrape more resutls\n",
    "        while total_count <= results_per_range: \n",
    "            # Checks for next token in json response\n",
    "            print(\"top of WHILE loop\")\n",
    "            if 'next_token' in json_obj_response['meta']: \n",
    "                \n",
    "                next_token: str = json_obj_response['meta']['next_token'] # assigns 'next_token' to next_token: str object for easy use\n",
    "\n",
    "                # Creates url and connects to endpoint then assignts the JSON API response to json_obj_esponse\n",
    "                json_obj_response = connect_to_endpoint(url[0], headers, url[1], next_token) # prints response code\n",
    "                print(f\"While loop enpoint called: index {i} / {len(start_times_list) - 1}\") # for quality control\n",
    "                \n",
    "                next_token = None # ensures next token does not get passed into another function call \n",
    "\n",
    "                if 'data' in json_obj_response:\n",
    "                    # Loops through dictionaries in json_obj_response and appends them to the main json file\n",
    "                    # This is done because while theoretically the entire item count be appended at once, certain python vectorizing methods might cause disagreeable types. Looping through items avoids this happening\n",
    "                    for item in json_obj_response['data']:\n",
    "                        json_obj_by_time_range[f'time_range_{i}']['data'].append(item)\n",
    "                    \n",
    "                    total_count += json_obj_response['meta']['result_count'] # increments the result count to match the total results currently aquired\n",
    "                    json_obj_by_time_range[f'time_range_{i}']['meta']['result_count'] = total_count # changes the result count 'key' to meet the result count of all data\n",
    "\n",
    "                    if json_obj_by_time_range[f'time_range_{i}']['meta']['result_count'] > results_per_range:\n",
    "                        del json_obj_by_time_range[f'time_range_{i}']['data'][results_per_range:]\n",
    "                        json_obj_by_time_range[f'time_range_{i}']['meta']['result_count'] = len(json_obj_by_time_range[f'time_range_{i}']['data'])\n",
    "\n",
    "                    print(f\"data key found and data appended: {total_count} / {results_per_range} tweets in this range scraped\") # quality control\n",
    "                    print(f\"list index {i} / {(len(start_times_list) - 1)}\")\n",
    "\n",
    "                else:\n",
    "                    print(\"empty next token\") # quality control\n",
    "                    print(f\"max results scraped: {total_count} / {results_per_range} tweets in this range scraped\") # quality control\n",
    "                    print(f\"list index {i} / {(len(start_times_list) - 1)} scraping over. Total tweets will be less than desired\")\n",
    "                    break\n",
    "            \n",
    "            else:\n",
    "                print(\"No more tweets to scrape, total tweets will be less than amount desired.\") # quality control\n",
    "                print(f\"total results {total_count}\") # quality control\n",
    "                next_token = None # ensures next token does not get passed into another function call\n",
    "                break # exits while loop and calls for the next time range or terminates entire process\n",
    "            \n",
    "\n",
    "            time.sleep(5) # for rate limit \n",
    "        total_tweets_for_func_call += len(json_obj_by_time_range[f'time_range_{i}']['data'])\n",
    "        print(f\"{total_tweets_for_func_call} tweets scraped in entire function call\")\n",
    "        time.sleep(5) # for rate limit\n",
    "\n",
    "            \n",
    "        json_to_file = json.dumps(json_obj_by_time_range) # converts results from above code to a serialized json obj\n",
    "        # Creates json file in directory of program and writes serialized json obj to the newly made file\n",
    "        with open(f\"data_range_{i}.json\", \"w\") as outfile:\n",
    "            outfile.write(json_to_file)\n",
    "        \n",
    "\n",
    "\"\"\"This function takes all of the json files of tweet data and creates a sorted dictionary of the most appearing tweets.\"\"\"\n",
    "\"\"\"A post OR a retweet counts as ONE occurence of a tweet.\"\"\"\n",
    "def analyze_top_appearing_tweets_in_data(min_int_of_json: int, max_int_of_json: int, num_of_top_tweets: int) -> dict: # max_int_of_json is the max int of json files in your directory, this function will paginate through them\n",
    "    tweet_metrics_dict: dict = dict()\n",
    "    for i in range(0, max_int_of_json + 1):\n",
    "\n",
    "        # opens json file and assigns serialized json data to data_file \n",
    "        f = open(f'data_range_{i}.json')\n",
    "        data_file = json.load(f)\n",
    "\n",
    "        for item in data_file[f'time_range_{i}']['data']: # loops through the tweets in json file and adds to a dict key of that tweet's id if the tweet appears\n",
    "            temp_dict: dict = dict()\n",
    "            # Increments each tweet id 'key' in tweet_metrics_dict by one for each appearance of a tweet or each retweet of that tweet\n",
    "            if 'referenced_tweets' in item and item['referenced_tweets'][0]['type'] == \"retweeted\":\n",
    "                original_tweet_id_from_retweet = item['referenced_tweets'][0]['id']\n",
    "                if original_tweet_id_from_retweet in tweet_metrics_dict:\n",
    "                    tweet_metrics_dict[original_tweet_id_from_retweet]['appearance_count'] += 1\n",
    "            elif 'referenced_tweets' not in item:\n",
    "                temp_dict['appearance_count'] = 1\n",
    "                temp_dict['created_at'] = item['created_at']\n",
    "                tweet_metrics_dict[original_tweet_id_from_retweet] = temp_dict\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        f.close() # closes the json file\n",
    "\n",
    "    #sorted_dict = {} # creates a dictionary to assist with sorting tweet id's by the most appearances\n",
    "    #sorted_keys = sorted(tweet_metrics_dict, key=tweet_metrics_dict.get, reverse=True)  # sorts the keys by highes value (most tweet appearances)\n",
    "\n",
    "    #for w in sorted_keys:\n",
    "        #sorted_dict[w] = tweet_metrics_dict[w] # sorts the dictionary by value from greates to least\n",
    "    # Sort the dictionary by the \"total_appearances_in_dataset\" value in descending order\n",
    "    sorted_dict = dict(sorted(tweet_metrics_dict.items(), key=lambda x: x[1]['appearance_count'], reverse=True))\n",
    "    final_dict = delete_after_num_in_dict(num_of_top_tweets, sorted_dict)\n",
    "    return final_dict # returns sorted dict\n",
    "\n",
    "def analyze_top_appearing_tweets_in_data_wrong_demo(max_int_of_json: int) -> dict: # max_int_of_json is the max int of json files in your directory, this function will paginate through them\n",
    "    tweet_metrics_dict: dict = dict()\n",
    "    for i in range(0, max_int_of_json + 1):\n",
    "\n",
    "        # opens json file and assigns serialized json data to data_file \n",
    "        f = open(f'data_range_{i}.json')\n",
    "        data_file = json.load(f)\n",
    "\n",
    "        for item in data_file[f'time_range_{i}']['data']: # loops through the tweets in json file and adds to a dict key of that tweet's id if the tweet appears\n",
    "            # Increments each tweet id 'key' in tweet_metrics_dict by one for each appearance of a tweet or each retweet of that tweet\n",
    "            if 'referenced_tweets' in item and item['referenced_tweets'][0]['type'] == \"retweeted\":\n",
    "                original_tweet_id_from_retweet = item['referenced_tweets'][0]['id']\n",
    "                if original_tweet_id_from_retweet in tweet_metrics_dict:\n",
    "                    tweet_metrics_dict[original_tweet_id_from_retweet] += 1\n",
    "                else:\n",
    "                    tweet_metrics_dict[original_tweet_id_from_retweet] = 1\n",
    "            else:\n",
    "                this_tweet_id = item['id']\n",
    "                if this_tweet_id in tweet_metrics_dict:\n",
    "                    tweet_metrics_dict[this_tweet_id] += 1\n",
    "                else:\n",
    "                    tweet_metrics_dict[this_tweet_id] = 1\n",
    "\n",
    "        f.close() # closes the json file\n",
    "\n",
    "    sorted_dict = {} # creates a dictionary to assist with sorting tweet id's by the most appearances\n",
    "    sorted_keys = sorted(tweet_metrics_dict, key=tweet_metrics_dict.get, reverse=True)  # sorts the keys by highes value (most tweet appearances)\n",
    "\n",
    "    for w in sorted_keys:\n",
    "        sorted_dict[w] = tweet_metrics_dict[w] # sorts the dictionary by value from greates to least\n",
    "    final = delete_after_num_in_dict(1100, sorted_dict)\n",
    "    return final # returns sorted dict\n",
    "\n",
    "\n",
    "def return_stats_of_file(num_of_file: int):\n",
    "    f = open(f'data_range_{num_of_file}.json')\n",
    "    data_file = json.load(f)\n",
    "    print(f\"There are {len(data_file[f'time_range_{num_of_file}']['data'])} tweets in this data file\")\n",
    "    print(data_file[f'time_range_{num_of_file}']['meta'])\n",
    "    print(data_file[f'time_range_{num_of_file}']['time'])\n",
    "    f.close()\n",
    "\n",
    "def delete_after_num_in_dict(num: int, dictionary_obj: dict):\n",
    "    count = 0\n",
    "    empty_dict = dict()\n",
    "    for item in dictionary_obj:\n",
    "        count += 1\n",
    "        if count > num:\n",
    "            return empty_dict\n",
    "        empty_dict[item] = dictionary_obj[item]\n",
    "\n",
    "def analyze_top_retweeted_tweets_in_data(min_int_of_json: int, max_int_of_json: int, num_of_top_tweets: int) -> dict: # max_int_of_json is the max int of json files in your directory, this function will paginate through them\n",
    "    tweet_metrics_dict: dict = dict()\n",
    "    tweet_metrics_dict['data'] = list()\n",
    "    for i in range(min_int_of_json, max_int_of_json + 1):\n",
    "\n",
    "        # opens json file and assigns serialized json data to data_file \n",
    "        f = open(f'data_range_{i}.json')\n",
    "        data_file = json.load(f)\n",
    "\n",
    "        for item in data_file[f'time_range_{i}']['data']: # loops through the tweets in json file and adds retweet count to a dict key of that tweet's id \n",
    "            # Creates a key for each unique tweet id and assigns its retweet count to that key\n",
    "            if 'referenced_tweets' in item and item['referenced_tweets'][0]['type'] == \"retweeted\":\n",
    "                original_tweet_id_from_retweet = item['referenced_tweets'][0]['id']\n",
    "                if original_tweet_id_from_retweet not in tweet_metrics_dict:\n",
    "                    retweet_count = item['public_metrics']['retweet_count']\n",
    "                    created_time = item['created_at']\n",
    "                    tweet_metrics_dict[original_tweet_id_from_retweet] = [retweet_count, created_time]\n",
    "            else:\n",
    "                this_tweet_id = item['id']\n",
    "                if this_tweet_id not in tweet_metrics_dict:\n",
    "                    created_time = item['created_at']\n",
    "                    tweet_metrics_dict[this_tweet_id] = [item['public_metrics']['retweet_count'], created_time]\n",
    "\n",
    "        f.close() # closes the json file\n",
    "\n",
    "    sorted_dict = {} # creates a dictionary to assist with sorting tweet id's by the most appearances\n",
    "    sorted_keys = sorted(tweet_metrics_dict, key=tweet_metrics_dict.get, reverse=True)  # sorts the keys by highes value (most retweets)\n",
    "\n",
    "    for w in sorted_keys:\n",
    "        sorted_dict[w] = tweet_metrics_dict[w] # sorts the dictionary by value from greates to least\n",
    "    final_dict = delete_after_num_in_dict(num_of_top_tweets, sorted_dict)\n",
    "    \n",
    "    #temp = len(final_dict)\n",
    "    #count = 0\n",
    "    #for item in final_dict:\n",
    "        #print(f\"{count} / {temp} item parsed\")\n",
    "        #count += 1\n",
    "        #final_dict[item][1] = get_created_at_of_tweet_id(item)\n",
    "    return final_dict # returns sorted dict\n",
    "\n",
    "\n",
    "\"\"\"Gathers num_of_results number of replies to a tweet from conversation_id.\"\"\"\n",
    "def json_replies_to_tweet_from_conversation_id(conversation_id: str, num_of_results: int):\n",
    "\n",
    "    # Make a request to the search endpoint to retrieve all tweets in the conversation\n",
    "    bearer_token = auth()\n",
    "    headers: dict[str, str] = create_headers(bearer_token)\n",
    "    total_replies_scraped: int = 0\n",
    "\n",
    "    response: requests.models.Response = requests.get(f\"https://api.twitter.com/2/tweets/search/all?max_results=100&query=conversation_id:{conversation_id} is:reply&tweet.fields=author_id,in_reply_to_user_id,created_at,conversation_id\", headers={\"Authorization\": f\"Bearer {bearer_token}\"})\n",
    "\n",
    "    # Extract the tweets from the response\n",
    "    reply_tweets: json = response.json()\n",
    "    tweets_temp: json = reply_tweets\n",
    "    total_replies_scraped = reply_tweets['meta']['result_count']\n",
    "\n",
    "    # Checks if more replies need to be scraped and finds the next token if true\"\n",
    "    while total_replies_scraped < num_of_results:\n",
    "        if 'next_token' in tweets_temp['meta']:\n",
    "            next_token1: str = tweets_temp['meta']['next_token']\n",
    "            response: requests.models.Response = requests.get(f\"https://api.twitter.com/2/tweets/search/all?max_results=100&next_token={next_token1}&query=conversation_id:{conversation_id} is:reply&tweet.fields=author_id,in_reply_to_user_id,created_at,conversation_id\", headers={\"Authorization\": f\"Bearer {bearer_token}\"})\n",
    "            tweets_temp = response.json()\n",
    "            if 'data' in tweets_temp:\n",
    "                total_replies_scraped += tweets_temp['meta']['result_count']\n",
    "                for item in tweets_temp['data']:\n",
    "                    reply_tweets['data'].append(item)\n",
    "                reply_tweets['meta']['result_count'] = total_replies_scraped\n",
    "                if reply_tweets['meta']['result_count'] > num_of_results:\n",
    "                    del reply_tweets['data'][num_of_results:]\n",
    "                    reply_tweets['meta']['result_count'] = len(reply_tweets['data'])\n",
    "            else:\n",
    "                print(\"empty next token\") # quality control\n",
    "                print(f\"max results scraped: {total_replies_scraped} / {num_of_results}\") # quality control\n",
    "                break\n",
    "        else:\n",
    "            print(\"No more tweets to scrape, total tweets will be less than amount desired.\") # quality control\n",
    "            print(f\"total results {total_replies_scraped}\") # quality control\n",
    "            break\n",
    "        time.sleep(5) # for rate limit \n",
    "\n",
    "        # Print the tweets\n",
    "    print(f\"tweet replies scraped/replies desired: {total_replies_scraped} / {num_of_results}\") # quality control\n",
    "    print(json.dumps(reply_tweets, indent=5))\n",
    "\n",
    "def get_username_from_tweet_id(tweet_id: str):\n",
    "    # Define the URL of the tweet endpoint\n",
    "    tweet_url = f'https://api.twitter.com/2/users?ids={get_author_id_from_tweet_id(tweet_id)}'\n",
    "\n",
    "    # Define your Twitter API credentials\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {auth()}',\n",
    "        'User-Agent': 'YOUR_APP_NAME',\n",
    "    }\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(tweet_url, headers=headers)\n",
    "    print(response.status_code)\n",
    "    time.sleep(3.5)\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Load the response data into a Python dictionary\n",
    "        tweet = json.loads(response.text)\n",
    "        if 'data' in tweet:\n",
    "            return(tweet['data'][0]['username'])\n",
    "        else:\n",
    "            return(f\"\")\n",
    "    else:\n",
    "        return(f\"\")\n",
    "\n",
    "def collect_replies_to_user_from_username(username: str, start_time: str, end_time: str, num_replies: int):\n",
    "    \n",
    "    user_name: str = username\n",
    "    query = f'to%3A{user_name}'\n",
    "    #query = 'to%3Adfreelon'\n",
    "    #query = from%3Adfreelon\n",
    "    tweet_fields = 'created_at,entities,public_metrics,id,text,lang'\n",
    "    user_fields = 'created_at,description,entities,id,location,name,public_metrics,username,verified'\n",
    "    #start_time = \"2022-03-02T05:00:00Z\"\n",
    "    #end_time = \"2022-03-06T14:00:00Z\"\n",
    "    expansions = 'referenced_tweets.id.author_id'\n",
    "\n",
    "    headers1 = create_headers(auth())\n",
    "\n",
    "    url = 'https://api.twitter.com/2/tweets/search/all?query=' + \\\n",
    "                    query + \\\n",
    "                    '&max_results=500&start_time=' + \\\n",
    "                    start_time + \\\n",
    "                    '&end_time=' + \\\n",
    "                    end_time + \\\n",
    "                    '&tweet.fields=' + \\\n",
    "                    tweet_fields + \\\n",
    "                    '&expansions=' + \\\n",
    "                    expansions + \\\n",
    "                    '&user.fields=' + \\\n",
    "                    user_fields \n",
    "\n",
    "    # Creates url and connects to endpoint then assignts the json object API response to json_obj_response\n",
    "    #reply_url = create_full_search_url('to%3Adfreelon', start_time, end_time, 100)\n",
    "    response = requests.request(\"GET\", url, headers = headers1)\n",
    "    json_response = response.json()\n",
    "    if 'meta' not in json_response:\n",
    "        print(json_response)\n",
    "        return json_response\n",
    "    else:\n",
    "        total_count = json_response['meta']['result_count']\n",
    "\n",
    "    while total_count <= (num_replies - 100): \n",
    "            # Checks for next token in json response\n",
    "            print(\"top of WHILE loop\")\n",
    "            \n",
    "            if 'next_token' in json_response['meta']: \n",
    "                \n",
    "                if (num_replies - total_count) < 500:\n",
    "                    max_nums = (num_replies - total_count)\n",
    "                else:\n",
    "                    max_nums = 500\n",
    "                next_token: str = json_response['meta']['next_token'] # assigns 'next_token' to next_token: str object for easy use\n",
    "            \n",
    "                url = 'https://api.twitter.com/2/tweets/search/all?query=' + \\\n",
    "                    query + \\\n",
    "                    f'&max_results={max_nums}&start_time=' + \\\n",
    "                    start_time + \\\n",
    "                    '&end_time=' + \\\n",
    "                    end_time + \\\n",
    "                    '&tweet.fields=' + \\\n",
    "                    tweet_fields + \\\n",
    "                    '&expansions=' + \\\n",
    "                    expansions + \\\n",
    "                    '&user.fields=' + \\\n",
    "                    user_fields + \\\n",
    "                    '&next_token=' + \\\n",
    "                    next_token\n",
    "\n",
    "                # Creates url and connects to endpoint then assignts the JSON API response to json_obj_esponse\n",
    "                response = requests.request(\"GET\", url, headers = headers1) # prints response code\n",
    "                json_response2 = response.json()\n",
    "                #json_response['data'].append(json_response2)\n",
    "                print(f\"While loop enpoint called\") # for quality control\n",
    "                \n",
    "                next_token = None # ensures next token does not get passed into another function call \n",
    "\n",
    "                if 'data' in json_response2:\n",
    "                    # Loops through dictionaries in json_obj_response and appends them to the main json file\n",
    "                    # This is done because while theoretically the entire item count be appended at once, certain python vectorizing methods might cause disagreeable types. Looping through items avoids this happening\n",
    "                    for item in json_response2['data']:\n",
    "                        json_response['data'].append(item)\n",
    "                    if 'includes' in json_response2:\n",
    "                        if 'users' in json_response2['includes']:\n",
    "                            for item in json_response2['includes']['users']:\n",
    "                                json_response['includes']['users'].append(item)\n",
    "                        if 'tweets' in json_response2['includes']:\n",
    "                            for item in json_response2['includes']['tweets']:\n",
    "                                json_response['includes']['tweets'].append(item)\n",
    "                        \n",
    "\n",
    "                    total_count += json_response2['meta']['result_count'] # increments the result count to match the total results currently aquired\n",
    "                    json_response['meta']['result_count'] = total_count # changes the result count 'key' to meet the result count of all data\n",
    "                    if 'meta' in json_response2:\n",
    "                        if 'next_token' in json_response2['meta']:\n",
    "                            json_response['meta']['next_token'] = json_response2['meta']['next_token']\n",
    "                    else:\n",
    "                        json_response['meta']['next_token'] = None\n",
    "\n",
    "\n",
    "                    if json_response['meta']['result_count'] > num_replies:\n",
    "                        del json_response['data'][num_replies:]\n",
    "                        json_response['meta']['result_count'] = len(json_response['data'])\n",
    "\n",
    "                    print(f\"data key found and data appended: {total_count} / {num_replies} tweets in this range scraped\") # quality control\n",
    "                    json_response2 = {}\n",
    "                    \n",
    "                else:\n",
    "                    print(\"empty next token\") # quality control\n",
    "                    print(f\"max results scraped: {total_count} / {num_replies} tweets in this range scraped\") # quality control\n",
    "                    print(f\"Total tweets will be less than desired\")\n",
    "                    break\n",
    "            \n",
    "            else:\n",
    "                print(\"No more tweets to scrape, total tweets will be less than amount desired.\") # quality control\n",
    "                print(f\"total results {total_count}\") # quality control\n",
    "                next_token = None # ensures next token does not get passed into another function call\n",
    "                break # exits while loop and calls for the next time range or terminates entire process\n",
    "            \n",
    "\n",
    "            time.sleep(5) # for rate limit \n",
    "    print(total_count)\n",
    "    if 'data' in json_response:\n",
    "        total_count = len(json_response['data'])\n",
    "    print(f\"{total_count} tweets scraped in entire function call\")\n",
    "    time.sleep(5) # for rate limit\n",
    "\n",
    "    # converts results from above code to a serialized json obj\n",
    "    return(json_response)\n",
    "\n",
    "def get_created_at_of_tweet_id(tweet_id: str):\n",
    "    \"\"\"Creates queries to find the author id of a tweet's author.\"\"\"\n",
    "    search_url: str = f\"https://api.twitter.com/2/tweets/{tweet_id}\" \n",
    "\n",
    "    # change params to desired params\n",
    "    query_params: dict = {'tweet.fields': 'created_at'}\n",
    "    b_tok = auth()\n",
    "    headers = create_headers(b_tok)\n",
    "    response = requests.request(\"GET\", search_url, headers = headers, params = query_params)\n",
    "    time.sleep(4)\n",
    "    print(\"\\nEndpoint Response Code: \" + str(response.status_code)) # prints the enpoint response code for debugging help\n",
    "    json_response = response.json()\n",
    "    if 'data' in json_response:\n",
    "        if 'created_at' in json_response['data']:\n",
    "            return(json_response['data']['created_at'])\n",
    "        else:\n",
    "            return(tweet_id)\n",
    "    else:\n",
    "        return(tweet_id)\n",
    "    \n",
    "def add_week_to_timestamp(timestamp):\n",
    "    # parse the input timestamp into a datetime object\n",
    "    dt = datetime.datetime.fromisoformat(timestamp)\n",
    "\n",
    "    # add one week to the datetime object\n",
    "    dt = dt + datetime.timedelta(weeks=1)\n",
    "\n",
    "    # return the resulting timestamp in the desired format\n",
    "    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "def creates_json_file_of_replies_to_top_users(dict_from_analyze_top_retweeted: dict):\n",
    "    json_replies_to_most_retweeted_users: json = dict()\n",
    "    count = 1\n",
    "    for item in dict_from_analyze_top_retweeted:\n",
    "        print(f\"{count} / {len(dict_from_analyze_top_retweeted)}\")\n",
    "        count += 1\n",
    "        username_of_tweet = dict_from_analyze_top_retweeted[item][2]\n",
    "        tweet_created_time = dict_from_analyze_top_retweeted[item][1]\n",
    "        one_week_after_tweet_created = add_week_to_timestamp(tweet_created_time)\n",
    "        response = collect_replies_to_user_from_username(username_of_tweet, tweet_created_time, one_week_after_tweet_created, 1000)\n",
    "        if 'meta' in response:\n",
    "            response['meta']['retweet_stats'] = (dict_from_analyze_top_retweeted[item])\n",
    "        json_replies_to_most_retweeted_users[username_of_tweet] = response\n",
    "    json_to_file = json.dumps(json_replies_to_most_retweeted_users)\n",
    "    with open('replies_to_100_most_retweeted_users.json', \"w\") as outfile:\n",
    "            outfile.write(json_to_file)\n",
    "\n",
    "def creates_json_file_of_replies_to_most_appearing_users(dict_from_analyze_top_retweeted: dict):\n",
    "    json_replies_to_most_appearing_users: json = dict()\n",
    "    count = 1\n",
    "    for item in dict_from_analyze_top_retweeted:\n",
    "        print(f\"{count} / {len(dict_from_analyze_top_retweeted)}\")\n",
    "        count += 1\n",
    "        username_of_tweet = dict_from_analyze_top_retweeted[item]['username']\n",
    "        tweet_created_time = dict_from_analyze_top_retweeted[item]['created_at']\n",
    "        one_week_after_tweet_created = add_week_to_timestamp(tweet_created_time)\n",
    "        response = collect_replies_to_user_from_username(username_of_tweet, tweet_created_time, one_week_after_tweet_created, 1000)\n",
    "        if 'meta' in response:\n",
    "            response['meta']['retweet_stats'] = (dict_from_analyze_top_retweeted[item])\n",
    "        json_replies_to_most_appearing_users[username_of_tweet] = response\n",
    "    json_to_file = json.dumps(json_replies_to_most_appearing_users)\n",
    "    with open('replies_to_100_most_appearing_users.json', \"w\") as outfile:\n",
    "            outfile.write(json_to_file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Working Cell\n",
    "2 cells below:  \n",
    "\n",
    "Cell 1: collects 10k tweets per time range from 100 random hour ranges over the last year.  \n",
    "The cell then collects the top 105 retweeted tweets from the data collected (if a tweet is a retweet it finds the id and retweet count of the original tweet id) and collects 1000 replies to those users (replies are from the created_at time of the tweet, and up to a week after the created_at time; if the tweet was a retweet, then the replies are from created_at time of that retweet but replies to the original user). \n",
    "\n",
    "Cell 2: does the same thing as cell one except it does not rank tweets by retweet counts, but rather by appearances in the original dataset of 1million tweets. It simply parses through the data and counts original tweets appearances or retweets of an original tweet as an appearance for that tweet. Replies and other forms of 'referenced_tweets' are not counted. A tweet is only counted/incremented if it is an original tweet or the dataset has a retweet of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This is and example function use which retrieves 10,000 tweets per hour range for 100 random hour ranges from the past year.\"\"\"\n",
    "start_list, end_list = return_n_random_hour_ranges_sorted(100)\n",
    "json_final_data = tweets_per_range(\"Smartchain OR Airdrop OR Crypto OR Nft\", start_list, end_list, 10000)\n",
    "\n",
    "\"\"\"This function use returns the most retweeted tweets between the hour ranges desired.\"\"\"\n",
    "top_retweeted_tweet_ids_sorted = analyze_top_retweeted_tweets_in_data(0, 99, 105)\n",
    "\n",
    "\n",
    "# This section of code loops through the most retweeted tweet ids and collects the username of the authors of the tweets\n",
    "# --- Section start ---\n",
    "counter = 0\n",
    "for item in top_retweeted_tweet_ids_sorted:\n",
    "    top_retweeted_tweet_ids_sorted[item].append(get_username_from_tweet_id(item))\n",
    "    counter += 1\n",
    "    print(f\"{counter} / {len(top_retweeted_tweet_ids_sorted)}\")\n",
    "# --- Section end ---\n",
    " \n",
    "# This section writes the tweet ids and usernames to a dictionary for storage\n",
    "# --- Section start ---\n",
    "json_2_file = json.dumps(top_retweeted_tweet_ids_sorted)\n",
    "with open(f\"top_105_retweeted_users.json\", \"w\") as outfile:\n",
    "    outfile.write(json_2_file)\n",
    "# --- Section end ---\n",
    "\n",
    "f = open('top_105_retweeted_users.json')\n",
    "data_file = json.load(f)\n",
    "f.close()\n",
    "creates_json_file_of_replies_to_top_users(data_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This is and example function use which retrieves 10,000 tweets per hour range for 100 random hour ranges from the past year.\"\"\"\n",
    "start_list, end_list = return_n_random_hour_ranges_sorted(100)\n",
    "json_final_data = tweets_per_range(\"Smartchain OR Airdrop OR Crypto OR Nft\", start_list, end_list, 10000)\n",
    "\n",
    "\"\"\"This function use returns the most retweeted tweets between the hour ranges desired.\"\"\"\n",
    "top_retweeted_tweet_ids_sorted = analyze_top_retweeted_tweets_in_data(0, 99, 105)\n",
    "\n",
    "top_appearing_tweet_ids = analyze_top_appearing_tweets_in_data(0, 99, 105)\n",
    "counter = 0\n",
    "for item in top_appearing_tweet_ids:\n",
    "    top_appearing_tweet_ids[item]['username'] = get_username_from_tweet_id(item)\n",
    "    counter += 1\n",
    "    print(f\"{counter} / {len(top_appearing_tweet_ids)}\")\n",
    "# --- Section end ---\n",
    " \n",
    "# This section writes the tweet ids and usernames to a dictionary for storage\n",
    "# --- Section start ---\n",
    "json_2_file = json.dumps(top_appearing_tweet_ids)\n",
    "with open(f\"most_100_appearing_tweet_ids.json\", \"w\") as outfile:\n",
    "    outfile.write(json_2_file)\n",
    "# --- Section end ---\n",
    "f = open('most_100_appearing_tweet_ids.json')\n",
    "data_file = json.load(f)\n",
    "f.close()\n",
    "creates_json_file_of_replies_to_most_appearing_users(data_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
